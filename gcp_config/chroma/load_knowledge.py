import os
# import shutil # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤ã®ãŸã‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â† ä¸è¦
import argparse
import yaml # YAMLèª­ã¿è¾¼ã¿ã®ãŸã‚ã«è¿½åŠ 
from dotenv import load_dotenv  # main.pyã¨åŒã˜.envèª­ã¿è¾¼ã¿æ©Ÿèƒ½ã‚’è¿½åŠ 
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
import chromadb
import chromadb.config
# streamlit import removed - main.pyã«åˆã‚ã›ã¦ã‚·ãƒ³ãƒ—ãƒ«åŒ–
import sys
import time
import logging
import hashlib
import json
from datetime import datetime, timedelta
import functools
import random
import threading

# ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    if os.name != 'nt':
        import fcntl
    else:
        fcntl = None
except ImportError:
    fcntl = None

try:
    if os.name == 'nt':
        import msvcrt
    else:
        msvcrt = None
except ImportError:
    msvcrt = None

# Load environment variables - main.pyã¨åŒã˜
load_dotenv()

# --- å®šæ•° (main.pyã¨åˆã‚ã›ã‚‹) ---
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªèº«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
KNOWLEDGE_BASE_DIR = os.path.join(SCRIPT_DIR, "knowledge_base")

# main.pyã¨çµ±ä¸€ã•ã‚ŒãŸå®šæ•°
CHROMA_COLLECTION_NAME = "bouldering_advice"
GEMINI_EMBEDDING_MODEL = "models/embedding-001"  # main.pyã¨çµ±ä¸€ï¼ˆ768æ¬¡å…ƒï¼‰
DEFAULT_RETRIEVAL_K = 3  # main.pyã¨çµ±ä¸€
ANALYSIS_INTERVAL_SEC = 0.5  # main.pyã¨çµ±ä¸€

# load_knowledge.pyå›ºæœ‰ã®å®šæ•°
SECRETS_FILE_PATH = os.path.join(SCRIPT_DIR, "secrets.yaml")
METADATA_FILE_PATH = os.path.join(SCRIPT_DIR, "knowledge_metadata.json")
LOCK_FILE_PATH = os.path.join(SCRIPT_DIR, "load_knowledge.lock")
BACKUP_DIR = os.path.join(SCRIPT_DIR, "backups")

# --- ã‚¨ãƒ©ãƒ¼åˆ†é¡ï¼ˆPhase 3: è©³ç´°åŒ–ï¼‰ ---
class ChromaDBError(Exception):
    """ChromaDBé–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
    pass

class ChromaDBConnectionError(ChromaDBError):
    """ChromaDBæ¥ç¶šã‚¨ãƒ©ãƒ¼"""
    pass

class ChromaDBTimeoutError(ChromaDBError):
    """ChromaDBã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼"""
    pass

class ChromaDBCapacityError(ChromaDBError):
    """ChromaDBå®¹é‡ã‚¨ãƒ©ãƒ¼"""
    pass

class GeminiAPIError(Exception):
    """Gemini APIé–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
    pass

class GeminiAPIQuotaError(GeminiAPIError):
    """Gemini APIã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼"""
    pass

class GeminiAPIRateLimitError(GeminiAPIError):
    """Gemini APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼"""
    pass

class GeminiAPIAuthError(GeminiAPIError):
    """Gemini APIèªè¨¼ã‚¨ãƒ©ãƒ¼"""
    pass

class ConfigurationError(Exception):
    """è¨­å®šé–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
    pass

class FileProcessingError(Exception):
    """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
    pass

class NetworkError(Exception):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
    pass

class PartialFailureError(Exception):
    """éƒ¨åˆ†çš„å¤±æ•—ã‚¨ãƒ©ãƒ¼"""
    def __init__(self, message, successful_items=None, failed_items=None):
        super().__init__(message)
        self.successful_items = successful_items or []
        self.failed_items = failed_items or []

# --- ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ ---
def retry(max_attempts=3, backoff_factor=2, exceptions=(Exception,)):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        wait_time = backoff_factor ** attempt + random.uniform(0, 1)
                        logging.warning(f"ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/{max_attempts}: {str(e)} (æ¬¡ã®è©¦è¡Œã¾ã§ {wait_time:.1f}ç§’å¾…æ©Ÿ)")
                        time.sleep(wait_time)
                    else:
                        logging.error(f"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•° ({max_attempts}) ã«é”ã—ã¾ã—ãŸ: {str(e)}")
            
            raise last_exception
        return wrapper
    return decorator

# --- é«˜åº¦ãªãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆPhase 3ï¼‰ ---
def adaptive_retry(max_attempts=3, base_backoff=2, max_backoff=60, exceptions=(Exception,)):
    """é©å¿œçš„ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ - ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã«å¿œã˜ãŸæˆ¦ç•¥"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        # ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã«å¿œã˜ãŸå¾…æ©Ÿæ™‚é–“ã®èª¿æ•´
                        wait_time = calculate_backoff_time(e, attempt, base_backoff, max_backoff)
                        
                        logging.warning(f"Retry {attempt + 1}/{max_attempts}: {type(e).__name__}: {str(e)} (waiting {wait_time:.1f}s)")
                        time.sleep(wait_time)
                        
                        # ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ã«å¯¾ã™ã‚‹è¿½åŠ å‡¦ç†
                        handle_retry_preparation(e)
                    else:
                        logging.error(f"Max retry attempts ({max_attempts}) reached: {type(e).__name__}: {str(e)}")
            
            raise last_exception
        return wrapper
    return decorator

def calculate_backoff_time(exception, attempt, base_backoff, max_backoff):
    """ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã«å¿œã˜ãŸãƒãƒƒã‚¯ã‚ªãƒ•æ™‚é–“ã‚’è¨ˆç®—"""
    if isinstance(exception, GeminiAPIRateLimitError):
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é•·ã‚ã®å¾…æ©Ÿ
        wait_time = min(base_backoff ** (attempt + 2), max_backoff)
    elif isinstance(exception, ChromaDBTimeoutError):
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ®µéšçš„ã«å¢—åŠ 
        wait_time = min(base_backoff ** (attempt + 1), max_backoff)
    elif isinstance(exception, NetworkError):
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯çŸ­ã‚ã®å¾…æ©Ÿ
        wait_time = min(base_backoff * (attempt + 1), max_backoff // 2)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
        wait_time = min(base_backoff ** attempt, max_backoff)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ã‚’è¿½åŠ 
    jitter = random.uniform(0, wait_time * 0.1)
    return wait_time + jitter

def handle_retry_preparation(exception):
    """ãƒªãƒˆãƒ©ã‚¤å‰ã®æº–å‚™å‡¦ç†"""
    if isinstance(exception, ChromaDBConnectionError):
        logging.info("Preparing for ChromaDB reconnection...")
        # æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®ã‚¯ãƒªã‚¢ãªã©
    elif isinstance(exception, GeminiAPIError):
        logging.info("Preparing for Gemini API retry...")
        # APIã‚­ãƒ¼ã®å†æ¤œè¨¼ãªã©

# --- YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã™ã‚‹é–¢æ•° ---
def load_secrets_from_yaml(file_path=SECRETS_FILE_PATH):
    """YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã™ã‚‹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            secrets = yaml.safe_load(f)
            if not secrets:
                print(f"è­¦å‘Š: {file_path} ãŒç©ºã‹ã€ç„¡åŠ¹ãªYAMLå½¢å¼ã§ã™ã€‚", file=sys.stderr)
                return

            # Gemini APIã‚­ãƒ¼ã‚’è¨­å®š (secrets['google']['gemini_api_key'] ãŒå­˜åœ¨ã™ã‚Œã°)
            gemini_key = secrets.get('google', {}).get('gemini_api_key')
            if gemini_key:
                if "GEMINI_API_KEY" not in os.environ:
                    os.environ["GEMINI_API_KEY"] = gemini_key
                    print(f"{file_path} ã‹ã‚‰ Gemini APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¾ã—ãŸã€‚")
                else:
                    print(f"ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã¯æ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚{file_path} ã®å€¤ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚")
            else:
                print(f"è­¦å‘Š: {file_path} ã« Gemini API ã‚­ãƒ¼ ('google.gemini_api_key') ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", file=sys.stderr)

            # ChromaDB URLã‚’è¨­å®š (secrets['chromadb']['url'] ãŒå­˜åœ¨ã™ã‚Œã°)
            chromadb_url = secrets.get('chromadb', {}).get('url')
            if chromadb_url:
                if "CHROMA_DB_URL" not in os.environ:
                    os.environ["CHROMA_DB_URL"] = chromadb_url
                    print(f"{file_path} ã‹ã‚‰ ChromaDB URL ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¾ã—ãŸã€‚")
                else:
                    print(f"ç’°å¢ƒå¤‰æ•° CHROMA_DB_URL ã¯æ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚{file_path} ã®å€¤ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚")
            else:
                 print(f"è­¦å‘Š: {file_path} ã« ChromaDB URL ('chromadb.url') ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", file=sys.stderr)

    except FileNotFoundError:
        print(f"è­¦å‘Š: {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯Streamlit Secretsã‚’ä½¿ç”¨ã—ã¾ã™ã€‚", file=sys.stderr)
    except yaml.YAMLError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ã®èª­ã¿è¾¼ã¿ä¸­ã«YAMLã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {file_path} ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)

# --- è¨­å®šç®¡ç†ï¼ˆmain.pyã«åˆã‚ã›ã¦ã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰ ---
def get_gemini_api_key():
    """Gemini APIã‚­ãƒ¼ã‚’å–å¾— - main.pyã¨åŒã˜æ–¹å¼"""
    return os.getenv("GEMINI_API_KEY")

def get_chromadb_url():
    """ChromaDB URLã‚’å–å¾— - main.pyã¨åŒã˜æ–¹å¼"""
    return os.getenv("CHROMA_DB_URL")

def validate_configuration():
    """è¨­å®šã®æ¤œè¨¼ - main.pyã¨åŒã˜æ–¹å¼"""
    gemini_api_key = get_gemini_api_key()
    chromadb_url = get_chromadb_url()
    
    if not gemini_api_key:
        raise ConfigurationError("GEMINI_API_KEY environment variable not set")
    
    if not chromadb_url:
        raise ConfigurationError("CHROMA_DB_URL environment variable not set")
    
    # APIã‚­ãƒ¼å½¢å¼ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    if not gemini_api_key.startswith('AI'):
        logging.warning("Gemini API key format may be incorrect")
    
    # ChromaDB URLå½¢å¼ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    if not (chromadb_url.startswith('http://') or chromadb_url.startswith('https://')):
        logging.warning("ChromaDB URL format may be incorrect")
    
    # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(KNOWLEDGE_BASE_DIR):
        raise ConfigurationError(f"Knowledge base directory not found: {KNOWLEDGE_BASE_DIR}")
    
    logging.info("âœ… Configuration validation passed")
    return {
        'gemini_api_key': mask_sensitive_info(gemini_api_key),
        'chromadb_url': mask_sensitive_info(chromadb_url),
        'knowledge_base_dir': KNOWLEDGE_BASE_DIR
    }

# --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿é–¢æ•° (å¤‰æ›´ãªã—) ---
def load_documents():
    """knowledge_baseãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    print(f"'{KNOWLEDGE_BASE_DIR}' ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        loader = DirectoryLoader(KNOWLEDGE_BASE_DIR, glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
        documents = loader.load()
        if not documents:
            print(f"è­¦å‘Š: '{KNOWLEDGE_BASE_DIR}' ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", file=sys.stderr)
            return []
        print(f"{len(documents)} å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return documents
    except Exception as e:
        print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
        return None

# --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†å‰²é–¢æ•° (å¤‰æ›´ãªã—) ---
def split_documents(documents):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹"""
    if not documents:
        return []
    print("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    print(f"{len(texts)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")
    return texts

# --- ãƒªãƒˆãƒ©ã‚¤ä»˜ãChromaDBæ“ä½œé–¢æ•°ï¼ˆPhase 3: å¼·åŒ–ç‰ˆï¼‰ ---
@adaptive_retry(max_attempts=5, base_backoff=2, max_backoff=60, 
                exceptions=(ChromaDBError, ChromaDBConnectionError, ChromaDBTimeoutError, NetworkError))
def connect_to_chromadb(chromadb_url):
    """ChromaDBã¸ã®æ¥ç¶šï¼ˆé«˜åº¦ãªãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰- Phase 3å¼·åŒ–ç‰ˆ"""
    if not chromadb_url:
        error = ChromaDBConnectionError("ChromaDB URL not configured")
        error_analyzer.record_error(error, {'function': 'connect_to_chromadb'})
        raise error
        
    try:
        # main.pyã¨åŒã˜æ¥ç¶šæ–¹å¼
        settings = chromadb.config.Settings(chroma_api_impl="rest")
        client = chromadb.HttpClient(host=chromadb_url, settings=settings)
        
        # æ¥ç¶šç¢ºèªï¼ˆmain.pyã«ã¯ãªã„ãŒã€load_knowledge.pyã§ã¯å¿…è¦ï¼‰
        try:
            client.heartbeat()
        except Exception as e:
            if "Could not connect" in str(e):
                raise ChromaDBConnectionError(f"ChromaDB server not reachable: {str(e)}")
            elif "timeout" in str(e).lower():
                raise ChromaDBTimeoutError(f"ChromaDB connection timeout: {str(e)}")
            else:
                raise ChromaDBError(f"ChromaDB connection failed: {str(e)}")
        
        logging.info("âœ… ChromaDB connection established successfully")
        return client
        
    except (ChromaDBError, ChromaDBConnectionError, ChromaDBTimeoutError):
        raise  # æ—¢ã«åˆ†é¡æ¸ˆã¿ã®ã‚¨ãƒ©ãƒ¼ã¯å†ç™ºç”Ÿ
    except Exception as e:
        # æœªåˆ†é¡ã®ã‚¨ãƒ©ãƒ¼ã‚’é©åˆ‡ã«åˆ†é¡
        if "network" in str(e).lower() or "connection" in str(e).lower():
            error = NetworkError(f"Network error during ChromaDB connection: {str(e)}")
        else:
            error = ChromaDBError(f"Unexpected ChromaDB connection error: {str(e)}")
        
        error_analyzer.record_error(error, {'function': 'connect_to_chromadb', 'url': chromadb_url})
        raise error

@adaptive_retry(max_attempts=5, base_backoff=1.5, max_backoff=30,
                exceptions=(GeminiAPIError, GeminiAPIRateLimitError, GeminiAPIQuotaError, GeminiAPIAuthError))
def initialize_embeddings(gemini_api_key):
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆé«˜åº¦ãªãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰- Phase 3å¼·åŒ–ç‰ˆ"""
    if not gemini_api_key:
        error = GeminiAPIAuthError("Gemini API key not configured")
        error_analyzer.record_error(error, {'function': 'initialize_embeddings'})
        raise error
        
    try:
        # main.pyã¨å®Œå…¨ã«åŒã˜å®Ÿè£…
        embeddings = GoogleGenerativeAIEmbeddings(
            model=GEMINI_EMBEDDING_MODEL,
            google_api_key=gemini_api_key
        )
        
        # åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        try:
            test_embedding = embeddings.embed_query("test")
            if not test_embedding or len(test_embedding) == 0:
                raise GeminiAPIError("Embedding initialization test failed: empty result")
        except Exception as e:
            error_message = str(e).lower()
            if "quota" in error_message or "limit" in error_message:
                raise GeminiAPIQuotaError(f"Gemini API quota exceeded: {str(e)}")
            elif "rate" in error_message:
                raise GeminiAPIRateLimitError(f"Gemini API rate limit: {str(e)}")
            elif "auth" in error_message or "key" in error_message:
                raise GeminiAPIAuthError(f"Gemini API authentication error: {str(e)}")
            else:
                raise GeminiAPIError(f"Gemini API error: {str(e)}")
        
        logging.info("âœ… Gemini embeddings initialized successfully")
        return embeddings
        
    except (GeminiAPIError, GeminiAPIRateLimitError, GeminiAPIQuotaError, GeminiAPIAuthError):
        raise  # æ—¢ã«åˆ†é¡æ¸ˆã¿ã®ã‚¨ãƒ©ãƒ¼ã¯å†ç™ºç”Ÿ
    except Exception as e:
        error = GeminiAPIError(f"Unexpected Gemini API error: {str(e)}")
        error_analyzer.record_error(error, {'function': 'initialize_embeddings'})
        raise error

# --- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é–¢æ•° (main.pyã®å®Ÿè£…ã«åˆã‚ã›ã‚‹) ---
@retry(max_attempts=3, backoff_factor=2, exceptions=(ChromaDBError,))
def get_or_create_collection(client, collection_name):
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å–å¾—ã¾ãŸã¯ä½œæˆï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰- main.pyã¨åŒã˜å®Ÿè£…"""
    try:
        # main.pyã¨å®Œå…¨ã«åŒã˜å®Ÿè£…
        return client.get_or_create_collection(name=collection_name)
    except Exception as e:
        raise ChromaDBError(f"Failed to get or create collection: {str(e)}")

@retry(max_attempts=3, backoff_factor=2, exceptions=(ChromaDBError,))
def add_documents_to_collection(collection, documents, metadatas, ids, embeddings):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰- main.pyã®å®Ÿè£…ã«åˆã‚ã›ã‚‹"""
    try:
        # main.pyã¨åŒã˜å½¢å¼ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
        collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids,
            embeddings=embeddings
        )
    except Exception as e:
        raise ChromaDBError(f"Failed to add documents to collection: {str(e)}")

@retry(max_attempts=3, backoff_factor=2, exceptions=(ChromaDBError,))
def delete_documents_from_collection(collection, ids):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å‰Šé™¤ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    try:
        collection.delete(ids=ids)
    except Exception as e:
        raise ChromaDBError(f"Failed to delete documents from collection: {str(e)}")

@retry(max_attempts=3, backoff_factor=2, exceptions=(ChromaDBError,))
def query_collection(collection, query_texts, n_results=DEFAULT_RETRIEVAL_K):
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚¯ã‚¨ãƒªï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰- main.pyã¨åŒã˜å®Ÿè£…"""
    try:
        # main.pyã¨åŒã˜ã‚¯ã‚¨ãƒªæ–¹å¼
        return collection.query(
            query_texts=query_texts,
            n_results=n_results
        )
    except Exception as e:
        raise ChromaDBError(f"Failed to query collection: {str(e)}")

# --- ãƒ­ã‚°è¨­å®š ---
def setup_logging(log_level='INFO'):
    """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
    log_file = os.path.join(SCRIPT_DIR, 'chroma_update.log')
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
    level = getattr(logging, log_level.upper(), logging.INFO)
    
    # ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã®è¨­å®š
    logger = logging.getLogger()
    logger.setLevel(level)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# --- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¯ãƒ©ã‚¹ ---
class PerformanceMonitor:
    def __init__(self):
        self.start_time = None
        self.metrics = {}
    
    def start(self, operation_name):
        """æ“ä½œã®é–‹å§‹æ™‚é–“ã‚’è¨˜éŒ²"""
        self.start_time = time.time()
        self.metrics[operation_name] = {'start': self.start_time}
        logging.info(f"é–‹å§‹: {operation_name}")
    
    def end(self, operation_name):
        """æ“ä½œã®çµ‚äº†æ™‚é–“ã‚’è¨˜éŒ²ã—ã€çµŒéæ™‚é–“ã‚’è¨ˆç®—"""
        if operation_name in self.metrics:
            end_time = time.time()
            elapsed = end_time - self.metrics[operation_name]['start']
            self.metrics[operation_name]['end'] = end_time
            self.metrics[operation_name]['elapsed'] = elapsed
            logging.info(f"å®Œäº†: {operation_name} ({elapsed:.2f}ç§’)")
            return elapsed
        return None
    
    def get_summary(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šçµæœã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        summary = {}
        total_time = 0
        for operation, data in self.metrics.items():
            if 'elapsed' in data:
                summary[operation] = f"{data['elapsed']:.2f}ç§’"
                total_time += data['elapsed']
        summary['total_time'] = f"{total_time:.2f}ç§’"
        return summary

# --- ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®— ---
def calculate_file_hash(file_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®MD5ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logging.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼ ({file_path}): {e}")
        return None

# --- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç† ---
def load_metadata():
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        if os.path.exists(METADATA_FILE_PATH):
            with open(METADATA_FILE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logging.warning(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return {}

def save_metadata(metadata):
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    try:
        with open(METADATA_FILE_PATH, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logging.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {METADATA_FILE_PATH}")
    except Exception as e:
        logging.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ ---
def health_check(client, embeddings):
    """ChromaDBã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
    logging.info("=== ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é–‹å§‹ ===")
    health_status = {
        'chromadb_connection': False,
        'collection_exists': False,
        'collection_count': 0,
        'embedding_test': False,
        'timestamp': datetime.now().isoformat()
    }
    
    try:
        # 1. ChromaDBæ¥ç¶šç¢ºèª
        client.heartbeat()
        health_status['chromadb_connection'] = True
        logging.info("âœ… ChromaDBæ¥ç¶š: OK")
        
        # 2. ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å­˜åœ¨ç¢ºèª
        try:
            collection = client.get_collection(CHROMA_COLLECTION_NAME)
            health_status['collection_exists'] = True
            health_status['collection_count'] = collection.count()
            logging.info(f"âœ… ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{CHROMA_COLLECTION_NAME}': å­˜åœ¨ ({health_status['collection_count']}ä»¶)")
        except Exception as e:
            logging.warning(f"âš ï¸ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{CHROMA_COLLECTION_NAME}': å­˜åœ¨ã—ãªã„ ({e})")
        
        # 3. åŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        try:
            test_text = "ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚"
            test_embedding = embeddings.embed_query(test_text)
            if test_embedding and len(test_embedding) > 0:
                health_status['embedding_test'] = True
                health_status['embedding_dimension'] = len(test_embedding)
                logging.info(f"âœ… åŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ: OK (æ¬¡å…ƒæ•°: {len(test_embedding)})")
            else:
                logging.error("âŒ åŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ: å¤±æ•— (ç©ºã®çµæœ)")
        except Exception as e:
            logging.error(f"âŒ åŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ: ã‚¨ãƒ©ãƒ¼ ({e})")
        
    except Exception as e:
        logging.error(f"âŒ ChromaDBæ¥ç¶š: ã‚¨ãƒ©ãƒ¼ ({e})")
    
    logging.info("=== ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Œäº† ===")
    return health_status

# --- æ©Ÿå¯†æƒ…å ±ãƒã‚¹ã‚­ãƒ³ã‚°æ©Ÿèƒ½ ---
def mask_sensitive_info(text, mask_char='*', visible_chars=4):
    """æ©Ÿå¯†æƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°"""
    if not text or len(text) <= visible_chars:
        return mask_char * 8
    return text[:visible_chars] + mask_char * (len(text) - visible_chars)

def safe_log_config():
    """æ©Ÿå¯†æƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã—ã¦è¨­å®šæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ› - main.pyã«åˆã‚ã›ã¦ã‚·ãƒ³ãƒ—ãƒ«åŒ–"""
    gemini_key = get_gemini_api_key()
    chroma_url = get_chromadb_url()
    
    logging.info("=== Configuration (Masked) ===")
    logging.info(f"Gemini API Key: {mask_sensitive_info(gemini_key) if gemini_key else 'âŒ Not set'}")
    logging.info(f"ChromaDB URL: {mask_sensitive_info(chroma_url) if chroma_url else 'âŒ Not set'}")
    logging.info(f"Gemini Embedding Model: {GEMINI_EMBEDDING_MODEL}")
    logging.info(f"Collection Name: {CHROMA_COLLECTION_NAME}")
    logging.info(f"Knowledge Base Directory: {KNOWLEDGE_BASE_DIR}")

# --- ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œçŸ¥æ©Ÿèƒ½ï¼ˆincrementalãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰ ---
def detect_file_changes():
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã‚’æ¤œçŸ¥ã—ã€æ›´æ–°ãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š"""
    metadata = load_metadata()
    current_files = {}
    changed_files = []
    
    if not os.path.exists(KNOWLEDGE_BASE_DIR):
        logging.error(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {KNOWLEDGE_BASE_DIR}")
        return []
    
    # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
    for filename in os.listdir(KNOWLEDGE_BASE_DIR):
        if filename.endswith('.txt'):
            file_path = os.path.join(KNOWLEDGE_BASE_DIR, filename)
            file_hash = calculate_file_hash(file_path)
            file_mtime = os.path.getmtime(file_path)
            
            current_files[filename] = {
                'hash': file_hash,
                'mtime': file_mtime,
                'size': os.path.getsize(file_path)
            }
    
    # å‰å›ã®æƒ…å ±ã¨æ¯”è¼ƒ
    last_files = metadata.get('file_hashes', {})
    
    for filename, current_info in current_files.items():
        if filename not in last_files:
            # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«
            changed_files.append({
                'filename': filename,
                'change_type': 'new',
                'current_info': current_info
            })
        elif current_info['hash'] != last_files[filename].get('hash'):
            # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
            changed_files.append({
                'filename': filename,
                'change_type': 'modified',
                'current_info': current_info,
                'previous_info': last_files[filename]
            })
    
    # å‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
    for filename in last_files:
        if filename not in current_files:
            changed_files.append({
                'filename': filename,
                'change_type': 'deleted',
                'previous_info': last_files[filename]
            })
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
    metadata['file_hashes'] = current_files
    metadata['last_file_check'] = datetime.now().isoformat()
    save_metadata(metadata)
    
    return changed_files

# --- è¨­å®šçµ±ä¸€åŒ–é–¢æ•°ï¼ˆmain.pyã«åˆã‚ã›ã¦ã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰ ---
def load_unified_config(config_file_path=None):
    """çµ±ä¸€ã•ã‚ŒãŸè¨­å®šèª­ã¿è¾¼ã¿ - main.pyã«åˆã‚ã›ã¦ç’°å¢ƒå¤‰æ•°å„ªå…ˆ"""
    config = {
        'gemini_api_key': get_gemini_api_key(),
        'chromadb_url': get_chromadb_url(),
        'collection_name': CHROMA_COLLECTION_NAME,
        'embedding_model': GEMINI_EMBEDDING_MODEL,
        'chunk_size': 500,
        'chunk_overlap': 50,
        'log_level': 'INFO'
    }
    
    return config

# --- åŒæ™‚å®Ÿè¡Œåˆ¶å¾¡ ---
class ProcessLock:
    """ãƒ—ãƒ­ã‚»ã‚¹ãƒ­ãƒƒã‚¯ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    def __init__(self, lock_file_path=LOCK_FILE_PATH):
        self.lock_file_path = lock_file_path
        self.lock_file = None
        self.is_locked = False
    
    def __enter__(self):
        return self.acquire()
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()
    
    def acquire(self):
        """ãƒ­ãƒƒã‚¯ã‚’å–å¾—"""
        try:
            self.lock_file = open(self.lock_file_path, 'w')
            
            if os.name == 'nt':  # Windows
                # Windowsã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯
                try:
                    msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_NBLCK, 1)
                    self.is_locked = True
                except IOError:
                    raise ProcessLock.LockError("åˆ¥ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œä¸­ã§ã™")
            else:  # Unix/Linux
                try:
                    fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                    self.is_locked = True
                except IOError:
                    raise ProcessLock.LockError("åˆ¥ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œä¸­ã§ã™")
            
            # ãƒ­ãƒƒã‚¯æƒ…å ±ã‚’æ›¸ãè¾¼ã¿
            lock_info = {
                'pid': os.getpid(),
                'timestamp': datetime.now().isoformat(),
                'command': ' '.join(sys.argv)
            }
            self.lock_file.write(json.dumps(lock_info, ensure_ascii=False, indent=2))
            self.lock_file.flush()
            
            logging.info(f"ãƒ—ãƒ­ã‚»ã‚¹ãƒ­ãƒƒã‚¯ã‚’å–å¾—ã—ã¾ã—ãŸ: {self.lock_file_path}")
            return self
            
        except Exception as e:
            if self.lock_file:
                self.lock_file.close()
            raise ProcessLock.LockError(f"ãƒ­ãƒƒã‚¯å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
    
    def release(self):
        """ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾"""
        if self.is_locked and self.lock_file:
            try:
                if os.name == 'nt':  # Windows
                    try:
                        msvcrt.locking(self.lock_file.fileno(), msvcrt.LK_UNLCK, 1)
                    except (OSError, IOError):
                        pass  # Windowsç’°å¢ƒã§ã®ãƒ­ãƒƒã‚¯è§£æ”¾ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
                else:  # Unix/Linux
                    try:
                        fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)
                    except (OSError, IOError):
                        pass  # Unixç’°å¢ƒã§ã®ãƒ­ãƒƒã‚¯è§£æ”¾ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
                
                self.lock_file.close()
                
                # ãƒ­ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                try:
                    if os.path.exists(self.lock_file_path):
                        os.remove(self.lock_file_path)
                except (OSError, IOError):
                    pass  # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
                
                logging.info("Process lock released successfully")
                self.is_locked = False
                
            except Exception as e:
                logging.warning(f"Lock release warning (non-critical): {str(e)}")
                self.is_locked = False
    
    class LockError(Exception):
        """ãƒ­ãƒƒã‚¯é–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
        pass

# --- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ ---
def create_backup():
    """ç¾åœ¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    logging.info("=== ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆé–‹å§‹ ===")
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
        logging.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {BACKUP_DIR}")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = os.path.join(BACKUP_DIR, f"chroma_backup_{timestamp}.json")
    
    try:
        # ChromaDBæ¥ç¶š
        gemini_api_key = get_gemini_api_key()
        chromadb_url = get_chromadb_url()
        
        if not gemini_api_key or not chromadb_url:
            raise ConfigurationError("APIã‚­ãƒ¼ã¾ãŸã¯ChromaDB URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ChromaDBæ¥ç¶šï¼ˆmain.pyã«åˆã‚ã›ã‚‹ï¼‰
        client = connect_to_chromadb(chromadb_url)
        
        # æ¥ç¶šç¢ºèª
        client.heartbeat()
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—
        try:
            collection = client.get_collection(CHROMA_COLLECTION_NAME)
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            results = collection.get(include=['documents', 'metadatas', 'embeddings'])
            
            backup_data = {
                'timestamp': timestamp,
                'collection_name': CHROMA_COLLECTION_NAME,
                'embedding_model': GEMINI_EMBEDDING_MODEL,
                'total_documents': len(results['documents']) if results['documents'] else 0,
                'data': {
                    'ids': results.get('ids', []),
                    'documents': results.get('documents', []),
                    'metadatas': results.get('metadatas', []),
                    # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¯å®¹é‡ãŒå¤§ãã„ãŸã‚ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§é™¤å¤–å¯èƒ½
                    'embeddings': results.get('embeddings', []) if len(results.get('embeddings', [])) < 1000 else []
                }
            }
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            file_size = os.path.getsize(backup_file) / (1024 * 1024)  # MB
            logging.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_file}")
            logging.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚µã‚¤ã‚º: {file_size:.2f} MB")
            logging.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»¶æ•°: {backup_data['total_documents']} ä»¶")
            
            return backup_file
            
        except Exception as e:
            raise ChromaDBError(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{CHROMA_COLLECTION_NAME}' ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«å¤±æ•—: {str(e)}")
            
    except Exception as e:
        logging.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

# --- ã‚¨ãƒ©ãƒ¼åˆ†æã¨ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ï¼ˆPhase 3ï¼‰ ---
class ErrorAnalyzer:
    """ã‚¨ãƒ©ãƒ¼åˆ†æã¨ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.error_history = []
        self.error_patterns = {}
        self.alert_thresholds = {
            'connection_failures': 3,
            'api_quota_errors': 5,
            'timeout_errors': 10,
            'critical_errors': 1
        }
    
    def record_error(self, error, context=None):
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²ã—åˆ†æ"""
        error_record = {
            'timestamp': datetime.now().isoformat(),
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context or {},
            'severity': self._classify_severity(error)
        }
        
        self.error_history.append(error_record)
        self._update_patterns(error_record)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆåˆ¤å®š
        if self._should_alert(error_record):
            self._send_alert(error_record)
        
        # ãƒ­ã‚°å‡ºåŠ›
        self._log_error_analysis(error_record)
    
    def _classify_severity(self, error):
        """ã‚¨ãƒ©ãƒ¼ã®é‡è¦åº¦ã‚’åˆ†é¡"""
        if isinstance(error, (GeminiAPIAuthError, ConfigurationError)):
            return 'CRITICAL'
        elif isinstance(error, (ChromaDBConnectionError, GeminiAPIQuotaError)):
            return 'HIGH'
        elif isinstance(error, (ChromaDBTimeoutError, GeminiAPIRateLimitError)):
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _update_patterns(self, error_record):
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ›´æ–°"""
        error_type = error_record['error_type']
        if error_type not in self.error_patterns:
            self.error_patterns[error_type] = {
                'count': 0,
                'first_seen': error_record['timestamp'],
                'last_seen': error_record['timestamp'],
                'frequency': []
            }
        
        pattern = self.error_patterns[error_type]
        pattern['count'] += 1
        pattern['last_seen'] = error_record['timestamp']
        pattern['frequency'].append(error_record['timestamp'])
        
        # éå»24æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿æŒ
        cutoff_time = datetime.now() - timedelta(hours=24)
        pattern['frequency'] = [
            ts for ts in pattern['frequency'] 
            if datetime.fromisoformat(ts) > cutoff_time
        ]
    
    def _should_alert(self, error_record):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡ã™ã¹ãã‹ã‚’åˆ¤å®š"""
        error_type = error_record['error_type']
        severity = error_record['severity']
        
        if severity == 'CRITICAL':
            return True
        
        # ã‚¨ãƒ©ãƒ¼é »åº¦ã«ã‚ˆã‚‹åˆ¤å®š
        pattern = self.error_patterns.get(error_type, {})
        recent_count = len(pattern.get('frequency', []))
        
        if error_type in ['ChromaDBConnectionError', 'NetworkError']:
            return recent_count >= self.alert_thresholds['connection_failures']
        elif error_type in ['GeminiAPIQuotaError']:
            return recent_count >= self.alert_thresholds['api_quota_errors']
        elif error_type in ['ChromaDBTimeoutError']:
            return recent_count >= self.alert_thresholds['timeout_errors']
        
        return False
    
    def _send_alert(self, error_record):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡"""
        alert_message = f"""
ğŸš¨ ALERT: {error_record['severity']} Error Detected
Error Type: {error_record['error_type']}
Message: {error_record['error_message']}
Time: {error_record['timestamp']}
Context: {error_record['context']}
        """.strip()
        
        logging.error(f"ALERT TRIGGERED: {alert_message}")
        
        # å®Ÿéš›ã®é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆSlackã€ãƒ¡ãƒ¼ãƒ«ãªã©ï¼‰ã¸ã®é€ä¿¡ã¯ã“ã“ã§å®Ÿè£…
        # self._send_to_notification_system(alert_message)
    
    def _log_error_analysis(self, error_record):
        """ã‚¨ãƒ©ãƒ¼åˆ†æçµæœã‚’ãƒ­ã‚°å‡ºåŠ›"""
        error_type = error_record['error_type']
        pattern = self.error_patterns.get(error_type, {})
        
        logging.info(f"Error Analysis - Type: {error_type}, "
                    f"Total Count: {pattern.get('count', 0)}, "
                    f"Recent Frequency: {len(pattern.get('frequency', []))}/24h, "
                    f"Severity: {error_record['severity']}")
    
    def get_error_summary(self):
        """ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        summary = {
            'total_errors': len(self.error_history),
            'error_types': {},
            'severity_distribution': {'CRITICAL': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0},
            'recent_trends': {}
        }
        
        for record in self.error_history:
            error_type = record['error_type']
            severity = record['severity']
            
            summary['error_types'][error_type] = summary['error_types'].get(error_type, 0) + 1
            summary['severity_distribution'][severity] += 1
        
        return summary

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¨ãƒ©ãƒ¼ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼
error_analyzer = ErrorAnalyzer()

# --- éƒ¨åˆ†çš„å¤±æ•—å¾©æ—§æ©Ÿèƒ½ï¼ˆPhase 3ï¼‰ ---
class PartialFailureRecovery:
    """éƒ¨åˆ†çš„å¤±æ•—ã‹ã‚‰ã®å¾©æ—§ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.failed_operations = []
        self.recovery_strategies = {
            'document_processing': self._recover_document_processing,
            'embedding_generation': self._recover_embedding_generation,
            'chromadb_insertion': self._recover_chromadb_insertion
        }
    
    def record_failure(self, operation_type, failed_items, context=None):
        """å¤±æ•—ã—ãŸæ“ä½œã‚’è¨˜éŒ²"""
        failure_record = {
            'timestamp': datetime.now().isoformat(),
            'operation_type': operation_type,
            'failed_items': failed_items,
            'context': context or {},
            'retry_count': 0,
            'recovered': False
        }
        
        self.failed_operations.append(failure_record)
        logging.warning(f"Partial failure recorded: {operation_type}, {len(failed_items)} items failed")
    
    def attempt_recovery(self, max_recovery_attempts=3):
        """å¤±æ•—ã—ãŸæ“ä½œã®å¾©æ—§ã‚’è©¦è¡Œ"""
        recovery_results = {
            'total_attempts': 0,
            'successful_recoveries': 0,
            'failed_recoveries': 0,
            'recovered_items': [],
            'permanently_failed_items': []
        }
        
        for failure_record in self.failed_operations:
            if failure_record['recovered'] or failure_record['retry_count'] >= max_recovery_attempts:
                continue
            
            operation_type = failure_record['operation_type']
            if operation_type in self.recovery_strategies:
                recovery_results['total_attempts'] += 1
                
                try:
                    recovered_items = self.recovery_strategies[operation_type](failure_record)
                    
                    if recovered_items:
                        failure_record['recovered'] = True
                        recovery_results['successful_recoveries'] += 1
                        recovery_results['recovered_items'].extend(recovered_items)
                        logging.info(f"Recovery successful: {operation_type}, {len(recovered_items)} items recovered")
                    else:
                        failure_record['retry_count'] += 1
                        recovery_results['failed_recoveries'] += 1
                        
                        if failure_record['retry_count'] >= max_recovery_attempts:
                            recovery_results['permanently_failed_items'].extend(failure_record['failed_items'])
                            logging.error(f"Recovery permanently failed: {operation_type}, {len(failure_record['failed_items'])} items")
                
                except Exception as e:
                    failure_record['retry_count'] += 1
                    recovery_results['failed_recoveries'] += 1
                    error_analyzer.record_error(e, {'operation_type': operation_type, 'recovery_attempt': True})
                    logging.error(f"Recovery attempt failed: {operation_type}, error: {e}")
        
        return recovery_results
    
    def _recover_document_processing(self, failure_record):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®å¾©æ—§"""
        recovered_items = []
        failed_items = failure_record['failed_items']
        
        for item in failed_items:
            try:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†å‡¦ç†ã®å®Ÿè£…
                if self._retry_document_processing(item):
                    recovered_items.append(item)
            except Exception as e:
                logging.warning(f"Document processing recovery failed for item {item}: {e}")
        
        return recovered_items
    
    def _recover_embedding_generation(self, failure_record):
        """åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã®å¾©æ—§"""
        recovered_items = []
        failed_items = failure_record['failed_items']
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã—ã¦å†è©¦è¡Œ
        small_batch_size = max(1, len(failed_items) // 4)
        
        for i in range(0, len(failed_items), small_batch_size):
            batch = failed_items[i:i + small_batch_size]
            try:
                if self._retry_embedding_generation(batch):
                    recovered_items.extend(batch)
            except Exception as e:
                logging.warning(f"Embedding generation recovery failed for batch: {e}")
        
        return recovered_items
    
    def _recover_chromadb_insertion(self, failure_record):
        """ChromaDBæŒ¿å…¥ã®å¾©æ—§"""
        recovered_items = []
        failed_items = failure_record['failed_items']
        
        # å€‹åˆ¥æŒ¿å…¥ã§å†è©¦è¡Œ
        for item in failed_items:
            try:
                if self._retry_chromadb_insertion(item):
                    recovered_items.append(item)
            except Exception as e:
                logging.warning(f"ChromaDB insertion recovery failed for item: {e}")
        
        return recovered_items
    
    def _retry_document_processing(self, item):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã®å†è©¦è¡Œ"""
        # å®Ÿè£…ã¯å…·ä½“çš„ãªå‡¦ç†å†…å®¹ã«ä¾å­˜
        return True
    
    def _retry_embedding_generation(self, batch):
        """åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã®å†è©¦è¡Œ"""
        # å®Ÿè£…ã¯å…·ä½“çš„ãªå‡¦ç†å†…å®¹ã«ä¾å­˜
        return True
    
    def _retry_chromadb_insertion(self, item):
        """ChromaDBæŒ¿å…¥ã®å†è©¦è¡Œ"""
        # å®Ÿè£…ã¯å…·ä½“çš„ãªå‡¦ç†å†…å®¹ã«ä¾å­˜
        return True
    
    def get_failure_summary(self):
        """å¤±æ•—ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        total_failures = len(self.failed_operations)
        recovered_failures = sum(1 for f in self.failed_operations if f['recovered'])
        pending_failures = total_failures - recovered_failures
        
        return {
            'total_failures': total_failures,
            'recovered_failures': recovered_failures,
            'pending_failures': pending_failures,
            'recovery_rate': recovered_failures / total_failures if total_failures > 0 else 0
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¾©æ—§ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
recovery_manager = PartialFailureRecovery()

# --- ãƒ¢ãƒƒã‚¯æ©Ÿèƒ½ï¼ˆé–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰ ---
class MockChromaClient:
    """ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¢ãƒƒã‚¯å®Ÿè£…"""
    
    def __init__(self):
        self.collections = {}
        self.is_mock = True
        logging.info("ğŸ­ Mock ChromaDB Client initialized")
    
    def heartbeat(self):
        """æ¥ç¶šç¢ºèªã®ãƒ¢ãƒƒã‚¯"""
        logging.info("ğŸ­ Mock ChromaDB heartbeat: OK")
        return True
    
    def get_or_create_collection(self, name):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—/ä½œæˆã®ãƒ¢ãƒƒã‚¯"""
        if name not in self.collections:
            self.collections[name] = MockCollection(name)
            logging.info(f"ğŸ­ Mock collection created: {name}")
        else:
            logging.info(f"ğŸ­ Mock collection retrieved: {name}")
        return self.collections[name]
    
    def get_collection(self, name):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ã®ãƒ¢ãƒƒã‚¯"""
        if name in self.collections:
            logging.info(f"ğŸ­ Mock collection found: {name}")
            return self.collections[name]
        else:
            raise Exception(f"Collection {name} not found")
    
    def delete_collection(self, name):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤ã®ãƒ¢ãƒƒã‚¯"""
        if name in self.collections:
            del self.collections[name]
            logging.info(f"ğŸ­ Mock collection deleted: {name}")
        else:
            logging.warning(f"ğŸ­ Mock collection not found for deletion: {name}")

class MockCollection:
    """ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ¢ãƒƒã‚¯å®Ÿè£…"""
    
    def __init__(self, name):
        self.name = name
        self.documents = []
        self.metadatas = []
        self.ids = []
        self.embeddings = []
        logging.info(f"ğŸ­ Mock collection '{name}' initialized")
    
    def add(self, documents, metadatas, ids, embeddings):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ã®ãƒ¢ãƒƒã‚¯"""
        self.documents.extend(documents)
        self.metadatas.extend(metadatas)
        self.ids.extend(ids)
        self.embeddings.extend(embeddings)
        logging.info(f"ğŸ­ Mock add: {len(documents)} documents added to '{self.name}'")
    
    def delete(self, ids):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã®ãƒ¢ãƒƒã‚¯"""
        logging.info(f"ğŸ­ Mock delete: {len(ids)} documents deleted from '{self.name}'")
    
    def query(self, query_texts, n_results=3):
        """ã‚¯ã‚¨ãƒªã®ãƒ¢ãƒƒã‚¯"""
        logging.info(f"ğŸ­ Mock query: {len(query_texts)} queries on '{self.name}', n_results={n_results}")
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        return {
            'ids': [self.ids[:n_results] if self.ids else []],
            'documents': [self.documents[:n_results] if self.documents else []],
            'metadatas': [self.metadatas[:n_results] if self.metadatas else []],
            'distances': [[0.1, 0.2, 0.3][:n_results]]
        }
    
    def count(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°å–å¾—ã®ãƒ¢ãƒƒã‚¯"""
        count = len(self.documents)
        logging.info(f"ğŸ­ Mock count: {count} documents in '{self.name}'")
        return count
    
    def get(self, include=None):
        """å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãƒ¢ãƒƒã‚¯"""
        logging.info(f"ğŸ­ Mock get: retrieving all data from '{self.name}'")
        result = {}
        if include is None or 'documents' in include:
            result['documents'] = self.documents
        if include is None or 'metadatas' in include:
            result['metadatas'] = self.metadatas
        if include is None or 'ids' in include:
            result['ids'] = self.ids
        if include is None or 'embeddings' in include:
            result['embeddings'] = self.embeddings
        return result

class MockEmbeddings:
    """GeminiåŸ‹ã‚è¾¼ã¿ã®ãƒ¢ãƒƒã‚¯å®Ÿè£…"""
    
    def __init__(self, model, google_api_key):
        self.model = model
        self.api_key = google_api_key
        self.is_mock = True
        logging.info(f"ğŸ­ Mock Embeddings initialized: {model}")
    
    def embed_query(self, text):
        """ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ã®ãƒ¢ãƒƒã‚¯"""
        # 768æ¬¡å…ƒã®ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆmodels/embedding-001ã«åˆã‚ã›ã‚‹ï¼‰
        import hashlib
        import struct
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ã‹ã‚‰æ±ºå®šçš„ãªãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()
        
        # 768æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        vector = []
        for i in range(768):
            # ãƒãƒƒã‚·ãƒ¥ãƒã‚¤ãƒˆã‚’å¾ªç’°ä½¿ç”¨ã—ã¦æµ®å‹•å°æ•°ç‚¹æ•°ã‚’ç”Ÿæˆ
            byte_idx = i % len(hash_bytes)
            value = struct.unpack('B', hash_bytes[byte_idx:byte_idx+1])[0]
            # -1.0 ã‹ã‚‰ 1.0 ã®ç¯„å›²ã«æ­£è¦åŒ–
            normalized_value = (value / 255.0) * 2.0 - 1.0
            vector.append(normalized_value)
        
        logging.debug(f"ğŸ­ Mock embedding generated for text: '{text[:50]}...' (768 dimensions)")
        return vector
    
    def embed_documents(self, texts):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸ‹ã‚è¾¼ã¿ã®ãƒ¢ãƒƒã‚¯"""
        return [self.embed_query(text) for text in texts]

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç†é–¢æ•° ---
def load_and_store_knowledge_http(mode='replace', dry_run=False, use_mock=False):
    """
    çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ChromaDBã«èª­ã¿è¾¼ã¿ãƒ»æ ¼ç´ã™ã‚‹ï¼ˆHTTPæ¥ç¶šç‰ˆï¼‰
    
    Args:
        mode (str): å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ('replace', 'append', 'verify', 'incremental', 'backup', 'config-test', 'mock-test')
        dry_run (bool): True ã®å ´åˆã€å®Ÿéš›ã®æ›´æ–°ã¯è¡Œã‚ãšã«å‡¦ç†å†…å®¹ã®ã¿è¡¨ç¤º
        use_mock (bool): True ã®å ´åˆã€ChromaDBã¨Geminiã‚’ãƒ¢ãƒƒã‚¯åŒ–
    """
    
    # è¨­å®šæ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰
    if mode == 'config-test':
        logging.info("=== Configuration Validation ===")
        
        # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
        gemini_key = os.getenv('GEMINI_API_KEY')
        chroma_url = os.getenv('CHROMA_DB_URL')
        
        if not gemini_key:
            logging.error("âŒ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        if not chroma_url:
            logging.error("âŒ CHROMA_DB_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
            
        logging.info(f"âœ… GEMINI_API_KEY: {'*' * (len(gemini_key) - 4) + gemini_key[-4:]}")
        logging.info(f"âœ… CHROMA_DB_URL: {chroma_url}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
        knowledge_files = get_knowledge_files()
        logging.info(f"âœ… çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {len(knowledge_files)}å€‹æ¤œå‡º")
        for file_path in knowledge_files:
            logging.info(f"   - {os.path.basename(file_path)}")
        
        # Phase 3 ã‚¨ãƒ©ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
        logging.info("âœ… Phase 3 ã‚¨ãƒ©ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ : æ­£å¸¸å‹•ä½œ")
        error_analyzer.log_error("TEST_ERROR", "Configuration test error", "MEDIUM")
        
        logging.info("=== Configuration Test Completed Successfully ===")
        return True
    
    # ãƒ¢ãƒƒã‚¯ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰
    if mode == 'mock-test':
        use_mock = True
        logging.info("ğŸ­ Mock Test Mode: ChromaDBã¨Geminiã‚’ãƒ¢ãƒƒã‚¯åŒ–ã—ã¾ã™")
    
    # ãƒ¢ãƒƒã‚¯ä½¿ç”¨æ™‚ã®é€šçŸ¥
    if use_mock:
        logging.info("ğŸ­ Mock Mode Enabled: å®Ÿéš›ã®ã‚µãƒ¼ãƒ“ã‚¹ã«ã¯æ¥ç¶šã—ã¾ã›ã‚“")
    
    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    chromadb_url = os.getenv('CHROMA_DB_URL')
    
    if not gemini_api_key:
        logging.error("GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False
    if not chromadb_url:
        logging.error("CHROMA_DB_URLç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False
    
    # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    if use_mock:
        client = MockChromaClient()
    else:
        try:
            settings = chromadb.config.Settings(
                allow_reset=True,
                anonymized_telemetry=False
            )
            client = chromadb.HttpClient(host=chromadb_url, settings=settings)
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            client.heartbeat()
            logging.info(f"ChromaDBã«æ­£å¸¸ã«æ¥ç¶šã—ã¾ã—ãŸ: {chromadb_url}")
        except Exception as e:
            error_analyzer.log_error("CHROMADB_CONNECTION", str(e), "CRITICAL")
            if not use_mock:
                logging.error(f"ChromaDBã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return False
    
    # GeminiåŸ‹ã‚è¾¼ã¿ã®åˆæœŸåŒ–
    if use_mock:
        embeddings = MockEmbeddings(GEMINI_EMBEDDING_MODEL, gemini_api_key)
    else:
        try:
            embeddings = GoogleGenerativeAIEmbeddings(
                model=GEMINI_EMBEDDING_MODEL,
                google_api_key=gemini_api_key
            )
            logging.info(f"GeminiåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: {GEMINI_EMBEDDING_MODEL}")
        except Exception as e:
            error_analyzer.log_error("GEMINI_API", str(e), "HIGH")
            if not use_mock:
                logging.error(f"GeminiåŸ‹ã‚è¾¼ã¿ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return False

    # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ
    monitor = PerformanceMonitor()
    
    try:
        with ProcessLock():
            if mode == 'config-test':
                return True  # æ—¢ã«å‡¦ç†æ¸ˆã¿
            elif mode == 'mock-test':
                return perform_mock_test(client, embeddings, dry_run)
            elif mode == 'verify':
                return perform_health_check(client, embeddings)
            elif mode == 'backup':
                return perform_backup()
            elif mode == 'incremental':
                return perform_incremental_update(client, embeddings, dry_run)
            else:
                return perform_full_update(client, embeddings, mode, dry_run)
                
    except ProcessLock.LockError as e:
        logging.error(f"Process lock error: {e}")
        return False
    except Exception as e:
        logging.error(f"Main process error: {e}")
        return False

def perform_mock_test(client, embeddings, dry_run=False):
    """ãƒ¢ãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    logging.info("=== Mock Test Mode ===")
    
    try:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆãƒ†ã‚¹ãƒˆ
        collection = client.get_or_create_collection(CHROMA_COLLECTION_NAME)
        logging.info(f"ğŸ­ Mock collection created: {CHROMA_COLLECTION_NAME}")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
        documents = load_documents()
        if not documents:
            logging.warning("No documents found to process")
            return True
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†å‰²
        split_docs = split_documents(documents)
        logging.info(f"Split into {len(split_docs)} chunks")
        
        if dry_run:
            logging.info(f"ğŸ” DRY RUN: Would process {len(split_docs)} document chunks in mock mode")
            return True
        
        # ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ 
        batch_size = 10  # ãƒ¢ãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ã«å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚º
        
        for i in range(0, min(len(split_docs), 20), batch_size):  # æœ€åˆã®20å€‹ã®ã¿ãƒ†ã‚¹ãƒˆ
            batch = split_docs[i:i + batch_size]
            
            # ãƒãƒƒãƒã®æº–å‚™
            batch_texts = [doc.page_content for doc in batch]
            batch_metadatas = [doc.metadata for doc in batch]
            batch_ids = [f"mock_doc_{i + j}" for j in range(len(batch))]
            
            # ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            batch_embeddings = embeddings.embed_documents(batch_texts)
            
            # ãƒ¢ãƒƒã‚¯ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
            collection.add(
                documents=batch_texts,
                metadatas=batch_metadatas,
                ids=batch_ids,
                embeddings=batch_embeddings
            )
            
            logging.info(f"ğŸ­ Mock processed batch {i//batch_size + 1}")
        
        # ãƒ¢ãƒƒã‚¯ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
        test_query = "ã‚¯ãƒ©ã‚¤ãƒŸãƒ³ã‚°ã®åŸºæœ¬çš„ãªå‹•ã"
        query_embedding = embeddings.embed_query(test_query)
        results = collection.query(
            query_texts=[test_query],
            n_results=3
        )
        
        logging.info(f"ğŸ­ Mock query test completed: found {len(results['ids'][0])} results")
        
        # çµæœç¢ºèª
        final_count = collection.count()
        logging.info(f"âœ… Mock test completed successfully")
        logging.info(f"âœ… Mock collection count: {final_count}")
        
        return True
        
    except Exception as e:
        logging.error(f"Mock test failed: {e}")
        return False

def perform_health_check(client, embeddings):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
    logging.info("=== Health Check Mode ===")
    
    try:
        # ChromaDBæ¥ç¶šãƒ†ã‚¹ãƒˆ
        if hasattr(client, 'is_mock') and client.is_mock:
            logging.info("ğŸ­ Mock ChromaDB connection: OK")
            chromadb_ok = True
        else:
            client.heartbeat()
            logging.info("âœ… ChromaDB connection: OK")
            chromadb_ok = True
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å­˜åœ¨ç¢ºèª
        try:
            collection = client.get_collection(CHROMA_COLLECTION_NAME)
            collection_exists = True
            doc_count = collection.count()
            logging.info(f"âœ… Collection exists: {CHROMA_COLLECTION_NAME} ({doc_count} documents)")
        except:
            collection_exists = False
            logging.warning(f"âš ï¸ Collection not found: {CHROMA_COLLECTION_NAME}")
        
        # åŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        if hasattr(embeddings, 'is_mock') and embeddings.is_mock:
            test_embedding = embeddings.embed_query("test")
            embedding_ok = len(test_embedding) == 768
            logging.info(f"ğŸ­ Mock embedding test: {'OK' if embedding_ok else 'FAILED'}")
        else:
            try:
                test_embedding = embeddings.embed_query("test")
                embedding_ok = len(test_embedding) > 0
                logging.info(f"âœ… Embedding test: OK ({len(test_embedding)} dimensions)")
            except Exception as e:
                embedding_ok = False
                logging.error(f"âŒ Embedding test failed: {e}")
        
        # ç·åˆçµæœ
        overall_health = chromadb_ok and collection_exists and embedding_ok
        logging.info(f"=== Health Check Result: {'âœ… HEALTHY' if overall_health else 'âŒ UNHEALTHY'} ===")
        
        return overall_health
        
    except Exception as e:
        logging.error(f"Health check failed: {e}")
        return False

def perform_backup():
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
    logging.info("=== Backup Mode ===")
    
    try:
        backup_file = create_backup()
        logging.info(f"âœ… Backup completed: {backup_file}")
        return True
    except Exception as e:
        logging.error(f"Backup failed: {e}")
        return False

def perform_incremental_update(client, embeddings, dry_run=False):
    """å¢—åˆ†æ›´æ–°ã‚’å®Ÿè¡Œ"""
    logging.info("=== Incremental Update Mode ===")
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œçŸ¥
        changed_files = detect_file_changes()
        
        if not changed_files:
            logging.info("No file changes detected. Nothing to update.")
            return True
        
        logging.info(f"Detected {len(changed_files)} file changes:")
        for change in changed_files:
            logging.info(f"  {change['change_type']}: {change['filename']}")
        
        if dry_run:
            logging.info("ğŸ” DRY RUN: Would process these file changes")
            return True
        
        # å®Ÿéš›ã®æ›´æ–°å‡¦ç†
        return process_file_changes(client, embeddings, changed_files)
        
    except Exception as e:
        logging.error(f"Incremental update failed: {e}")
        return False

def perform_full_update(client, embeddings, mode, dry_run=False):
    """å®Œå…¨æ›´æ–°ã‚’å®Ÿè¡Œ"""
    logging.info(f"=== Full Update Mode: {mode} ===")
    monitor = PerformanceMonitor()
    
    try:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—/ä½œæˆ
        monitor.start("Collection Setup")
        collection = client.get_or_create_collection(CHROMA_COLLECTION_NAME)
        monitor.end("Collection Setup")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
        monitor.start("Document Loading")
        documents = load_documents()
        if not documents:
            logging.warning("No documents found to process")
            return True
        monitor.end("Document Loading")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†å‰²
        monitor.start("Document Splitting")
        split_docs = split_documents(documents)
        logging.info(f"Split into {len(split_docs)} chunks")
        monitor.end("Document Splitting")
        
        if dry_run:
            logging.info(f"ğŸ” DRY RUN: Would process {len(split_docs)} document chunks")
            return True
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼ˆreplaceãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
        if mode == 'replace':
            monitor.start("Collection Clear")
            try:
                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¦å†ä½œæˆ
                client.delete_collection(CHROMA_COLLECTION_NAME)
                collection = client.get_or_create_collection(CHROMA_COLLECTION_NAME)
                logging.info("Collection cleared for replace mode")
            except Exception as e:
                logging.warning(f"Collection clear warning: {e}")
            monitor.end("Collection Clear")
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ 
        monitor.start("Document Processing")
        batch_size = 50
        
        for i in range(0, len(split_docs), batch_size):
            batch = split_docs[i:i + batch_size]
            
            # ãƒãƒƒãƒã®æº–å‚™
            batch_texts = [doc.page_content for doc in batch]
            batch_metadatas = [doc.metadata for doc in batch]
            batch_ids = [f"doc_{i + j}" for j in range(len(batch))]
            
            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            batch_embeddings = embeddings.embed_documents(batch_texts)
            
            # ChromaDBã«è¿½åŠ 
            collection.add(
                documents=batch_texts,
                metadatas=batch_metadatas,
                ids=batch_ids,
                embeddings=batch_embeddings
            )
            
            logging.info(f"Processed batch {i//batch_size + 1}/{(len(split_docs) + batch_size - 1)//batch_size}")
        
        monitor.end("Document Processing")
        
        # çµæœç¢ºèª
        final_count = collection.count()
        logging.info(f"âœ… Successfully processed {len(split_docs)} documents")
        logging.info(f"âœ… Final collection count: {final_count}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœè¡¨ç¤º
        performance_summary = monitor.get_summary()
        logging.info("=== Performance Summary ===")
        for operation, duration in performance_summary.items():
            logging.info(f"  {operation}: {duration}")
        
        return True
        
    except Exception as e:
        logging.error(f"Full update failed: {e}")
        return False

def process_file_changes(client, embeddings, changed_files):
    """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’å‡¦ç†"""
    logging.info("Processing file changes...")
    
    try:
        # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦å®Œå…¨æ›´æ–°ã‚’å®Ÿè¡Œ
        return perform_full_update(client, embeddings, 'replace', dry_run=False)
    except Exception as e:
        logging.error(f"File change processing failed: {e}")
        return False

if __name__ == "__main__":
    # --- ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ ---
    parser = argparse.ArgumentParser(description="çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã€ãƒªãƒ¢ãƒ¼ãƒˆChromaDBã«æ ¼ç´/è¿½åŠ ã—ã¾ã™ã€‚")
    parser.add_argument(
        "-m", "--mode",
        choices=['replace', 'append', 'verify', 'incremental', 'backup', 'config-test', 'mock-test'],
        default='replace',
        help="DBæ›´æ–°ãƒ¢ãƒ¼ãƒ‰ ('replace': å…¨ç½®ãæ›ãˆ, 'append': æ—¢å­˜ã«è¿½åŠ , 'verify': ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯, 'incremental': å¤‰æ›´åˆ†ã®ã¿æ›´æ–°, 'backup': ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ, 'config-test': è¨­å®šæ¤œè¨¼ã®ã¿, 'mock-test': ãƒ¢ãƒƒã‚¯ç’°å¢ƒã§ã®å®Œå…¨ãƒ†ã‚¹ãƒˆ)"
    )
    parser.add_argument(
        "--log-level",
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help="ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: INFO)"
    )
    parser.add_argument(
        "--dry-run",
        action='store_true',
        help="å®Ÿéš›ã®å¤‰æ›´ã‚’è¡Œã‚ãšã«å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"
    )
    parser.add_argument(
        "--config-file",
        default=SECRETS_FILE_PATH,
        help=f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {SECRETS_FILE_PATH})"
    )
    parser.add_argument(
        "--mock-chromadb",
        action='store_true',
        help="ChromaDBã‚’ãƒ¢ãƒƒã‚¯åŒ–ã—ã¦æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚’å›é¿"
    )
    args = parser.parse_args()

    # --- ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ– ---
    logger = setup_logging(args.log_level)
    
    # --- YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€ ---
    load_secrets_from_yaml(args.config_file)

    logging.info(f"=== çŸ¥è­˜ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ãƒ»æ ¼ç´ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ ===")
    logging.info(f"ãƒ¢ãƒ¼ãƒ‰: {args.mode}")
    logging.info(f"æ¥ç¶šå…ˆ: ãƒªãƒ¢ãƒ¼ãƒˆ ChromaDB")
    logging.info(f"ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: {args.log_level}")
    if args.dry_run:
        logging.info("ğŸ” DRY RUN ãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®å¤‰æ›´ã¯è¡Œã„ã¾ã›ã‚“")

    # è¨­å®šæ¤œè¨¼ã®å®Ÿè¡Œ
    logging.info("=== Configuration Validation ===")
    try:
        validate_configuration()
        logging.info("âœ… Configuration validation passed")
    except ConfigurationError as e:
        logging.error(f"âŒ Configuration error: {e}")
        sys.exit(1)
    
    # è¨­å®šæƒ…å ±ã®è¡¨ç¤ºï¼ˆæ©Ÿå¯†æƒ…å ±ãƒã‚¹ã‚­ãƒ³ã‚°ï¼‰
    safe_log_config()
    logging.info(f"Config file: {args.config_file}")
    logging.info("=" * 50)

    # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ
    success = load_and_store_knowledge_http(mode=args.mode, dry_run=args.dry_run, use_mock=args.mock_chromadb)

    if success:
        logging.info(f"=== å‡¦ç†å®Œäº† (ãƒ¢ãƒ¼ãƒ‰: {args.mode}) ===")
        if args.mode == 'verify':
            logging.info("ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
    else:
        logging.error(f"=== ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚çµ‚äº†ã—ã¾ã—ãŸ (ãƒ¢ãƒ¼ãƒ‰: {args.mode}) ===")
        sys.exit(1) 