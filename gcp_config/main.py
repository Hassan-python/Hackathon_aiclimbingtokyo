from fastapi import FastAPI, UploadFile, HTTPException, Header, File, Form, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.responses import FileResponse
from pydantic import BaseModel
import os
import uuid
from typing import Optional, List, Dict, Any, Tuple
from moviepy.editor import VideoFileClip
import cv2
import numpy as np
from dotenv import load_dotenv
import google.generativeai as genai
import chromadb
from chromadb.config import Settings
from PIL import Image
from google.cloud import storage
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from functools import lru_cache
from datetime import datetime, timedelta
import subprocess
import base64
from pathlib import Path
import logging

# Load environment variables
load_dotenv()

# Constants from environment variables (å®šç¾©ã‚’å…ˆã«ç§»å‹•)
GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME", "climbing-videos-bucket-climbing-application-458609")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
CHROMA_DB_URL = os.getenv("CHROMA_DB_URL")
CHROMA_COLLECTION_NAME = os.getenv("CHROMA_COLLECTION_NAME", "bouldering_advice")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "models/embedding-001")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
MEMORY_LIMIT = os.getenv("MEMORY_LIMIT", "4096M")
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "900"))
MAX_FILE_SIZE = os.getenv("MAX_FILE_SIZE", "100MB")
PHASE = os.getenv("PHASE", "2")

# Configure logging with environment variable
log_level = getattr(logging, LOG_LEVEL.upper() if LOG_LEVEL else "INFO", logging.INFO)
logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI()

# CORS debugging middleware
@app.middleware("http")
async def cors_debug_middleware(request, call_next):
    origin = request.headers.get("origin")
    logger.info(f"Request from origin: {origin}")
    logger.info(f"Request method: {request.method}")
    logger.info(f"Request URL: {request.url}")
    logger.info(f"Request headers: {dict(request.headers)}")
    
    response = await call_next(request)
    
    logger.info(f"Response status: {response.status_code}")
    logger.info(f"Response headers: {dict(response.headers)}")
    
    # Add additional CORS headers for debugging - ãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒã®URLã‚‚è¿½åŠ 
    allowed_origins = [
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "https://climbing-web-app-bolt-932280363930.asia-northeast1.run.app",
        "https://aiclimbingtokyo.com",  # ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®URL
        "https://www.aiclimbingtokyo.com",  # wwwãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        "https://gorgeous-sawine-8ea61b.netlify.app"  # Netlifyãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
    ]
    
    if origin and origin in allowed_origins:
        response.headers["Access-Control-Allow-Origin"] = origin
        response.headers["Access-Control-Allow-Credentials"] = "true"
        response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "Accept, Accept-Language, Content-Language, Content-Type, X-Language, Authorization, X-Gemini-Key, X-OpenAI-Key, X-OpenAI-Model"
        response.headers["Access-Control-Expose-Headers"] = "*"
    
    return response

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",  # Viteçµ±ä¸€ãƒãƒ¼ãƒˆ
        "http://127.0.0.1:5173",
        "https://climbing-web-app-bolt-932280363930.asia-northeast1.run.app",  # Production backend URL
        "https://aiclimbingtokyo.com",  # ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®URL
        "https://www.aiclimbingtokyo.com",  # wwwãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        "https://gorgeous-sawine-8ea61b.netlify.app"  # Netlifyãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=[
        "Accept",
        "Accept-Language", 
        "Content-Language",
        "Content-Type",
        "X-Language",
        "Authorization",
        "X-Gemini-Key",
        "X-OpenAI-Key", 
        "X-OpenAI-Model"
    ],
    expose_headers=["*"],
)

# Application constants
ANALYSIS_INTERVAL_SEC = 0.5
MAX_FRAMES_FOR_GEMINI = 10
DEFAULT_RETRIEVAL_K = 3
UPLOAD_DIR = Path("/tmp/videos")

# Ensure upload directory exists
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

class AnalysisSettings(BaseModel):
    problemType: str
    crux: str
    startTime: float
    gcsBlobName: str

class Source(BaseModel):
    name: str
    content: str

class AnalysisResponse(BaseModel):
    advice: str
    sources: list[Source]
    geminiAnalysis: Optional[str] = None

# æ–°ã—ã„æ©Ÿèƒ½ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«è¿½åŠ 
class VideoMetadata(BaseModel):
    originalFileName: str
    originalSize: int
    originalDuration: float
    optimizedSize: int
    optimizedDuration: float
    compressionRatio: float

class FullVideoUploadResponse(BaseModel):
    gcsBlobName: str
    videoId: str
    metadata: VideoMetadata
    previewUrl: str

class RangeAnalysisSettings(BaseModel):
    problemType: str
    crux: str
    startTime: float
    endTime: float
    gcsBlobName: str

# GCS Signed URLé–¢é€£ã®ãƒ¢ãƒ‡ãƒ«
class SignedUrlRequest(BaseModel):
    filename: str
    contentType: str

class SignedUrlResponse(BaseModel):
    uploadUrl: str
    gcsBlobName: str
    videoId: str

class VideoProcessRequest(BaseModel):
    gcsBlobName: str
    originalFileName: str

# ãƒ­ã‚°é–¢é€£ã®ãƒ¢ãƒ‡ãƒ«
class LogEntry(BaseModel):
    timestamp: str
    severity: str
    message: str

class LogResponse(BaseModel):
    logs: List[LogEntry]
    total_count: int

def validate_environment_variables():
    """èµ·å‹•æ™‚ã«å¿…é ˆç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    required_vars = {
        "GCS_BUCKET_NAME": "GCS bucket name for video storage",
        "GEMINI_API_KEY": "Gemini API key for AI analysis",
        "CHROMA_DB_URL": "ChromaDB server URL for knowledge retrieval"
    }
    
    missing_vars = []
    for var_name, description in required_vars.items():
        if not os.getenv(var_name):
            missing_vars.append(f"{var_name} ({description})")
    
    if missing_vars:
        error_msg = f"Missing required environment variables: {', '.join(missing_vars)}"
        logger.error(f"âŒ STARTUP ERROR: {error_msg}")
        raise RuntimeError(error_msg)
    
    logger.info("âœ… All required environment variables are configured")
    logger.info(f"Environment: PHASE={PHASE}, MEMORY_LIMIT={MEMORY_LIMIT}, REQUEST_TIMEOUT={REQUEST_TIMEOUT}")
    logger.info(f"ChromaDB: Collection={CHROMA_COLLECTION_NAME}, Embedding={EMBEDDING_MODEL}")
    logger.info(f"GCS Bucket: {GCS_BUCKET_NAME}")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯
try:
    validate_environment_variables()
except RuntimeError as e:
    print(f"Application startup failed: {e}")
    # æœ¬ç•ªç’°å¢ƒã§ã¯ sys.exit(1) ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¤œè¨
    pass

def extract_frames(video_path: str, start_sec: float, end_sec: float, interval_sec: float = ANALYSIS_INTERVAL_SEC) -> list:
    frames = []
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        raise HTTPException(status_code=400, detail="Could not open video file")
        
    fps = cap.get(cv2.CAP_PROP_FPS)
    start_frame = int(start_sec * fps)
    end_frame = int(end_sec * fps)
    interval_frames = max(1, int(interval_sec * fps))
    
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    
    current_frame = start_frame
    while current_frame <= end_frame:
        ret, frame = cap.read()
        if not ret:
            break
            
        if (current_frame - start_frame) % interval_frames == 0:
            frames.append(frame)
            
        current_frame += 1
        
    cap.release()
    return frames

@lru_cache(maxsize=1)
def get_chroma_client():
    """ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆå¤–éƒ¨ã‚µãƒ¼ãƒãƒ¼å¯¾å¿œï¼‰"""
    chromadb_url = os.getenv("CHROMA_DB_URL")
    if not chromadb_url:
        raise HTTPException(status_code=500, detail="ChromaDB URL not configured")
        
    try:
        settings = chromadb.config.Settings(chroma_api_impl="rest")
        client = chromadb.HttpClient(host=chromadb_url, settings=settings)
        
        # æ¥ç¶šç¢ºèª
        client.heartbeat()
        logger.info(f"ChromaDB HttpClient initialized: {chromadb_url}")
        
        return client
    except Exception as e:
        logger.error(f"ChromaDB connection failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ChromaDB connection failed: {str(e)}")

@lru_cache(maxsize=1)
def get_langchain_chroma_vectorstore() -> Chroma:
    """LangchainçµŒç”±ã§Chromaãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’å–å¾—ã™ã‚‹ï¼ˆå¤–éƒ¨ChromaDBã‚µãƒ¼ãƒãƒ¼å¯¾å¿œï¼‰"""
    try:
        gemini_embeddings = GoogleGenerativeAIEmbeddings(
            model=EMBEDDING_MODEL,
            google_api_key=GEMINI_API_KEY
        )
        
        chroma_client = get_chroma_client() 
        
        vectorstore = Chroma(
            client=chroma_client,
            collection_name=CHROMA_COLLECTION_NAME,
            embedding_function=gemini_embeddings
        )
        logger.info(f"Langchain Chroma vectorstore initialized with model: {EMBEDDING_MODEL}")
        return vectorstore
    except Exception as e:
        logger.error(f"Error creating Langchain Chroma vectorstore: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to initialize vectorstore: {str(e)}")

def retrieve_from_chroma_langchain(query: str, k: int = DEFAULT_RETRIEVAL_K) -> List[dict]:
    """Langchainãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ChromaDBã‹ã‚‰é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹"""
    try:
        vectorstore = get_langchain_chroma_vectorstore()
        source_docs_with_scores = vectorstore.similarity_search_with_score(query, k=k)
        
        documents = []
        print(f"[DEBUG Langchain Chroma] Executing query: {query} with k={k}")
        for i, (doc, score) in enumerate(source_docs_with_scores):
            doc_name = doc.metadata.get("name", f"doc_{i+1}") if doc.metadata else f"doc_{i+1}"
            documents.append({
                "name": doc_name,
                "content": doc.page_content,
                "score": score
            })
            print(f"[DEBUG Langchain Chroma] Retrieved doc: {doc_name}, Score: {score:.4f}, Content (first 50 chars): {doc.page_content[:50]}...")
        
        return documents
    except Exception as e:
        print(f"Langchain Chroma retrieval error: {e}")
        return []

def analyze_and_generate_advice(
    frames: list, 
    problem_type: str, 
    crux: str, 
    output_language: str
) -> Tuple[str, str, List[Source]]:
    """1å›ã®Geminiå‘¼ã³å‡ºã—ã§å‹•ç”»åˆ†æã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆã‚’è¡Œã†"""
    if not frames:
        return "No frames available for analysis", "ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“", []
        
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.0-flash-lite')
        
        # Select frames for analysis
        num_frames = min(len(frames), MAX_FRAMES_FOR_GEMINI)
        indices = np.linspace(0, len(frames) - 1, num_frames, dtype=int)
        selected_frames = [frames[i] for i in indices]
        
        # Convert frames to PIL images
        pil_images = []
        for frame in selected_frames:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_frame)
            pil_images.append(pil_image)
            
        # ChromaDBã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®ã¿ã‚’ä½¿ç”¨)
        rag_query = f"èª²é¡Œã®ç¨®é¡: {problem_type}, é›£ã—ã„ç‚¹: {crux}"
        print(f"[DEBUG] RAG query: {rag_query}")
        retrieved_docs_for_gemini = retrieve_from_chroma_langchain(rag_query)
        print(f"[DEBUG] Retrieved {len(retrieved_docs_for_gemini)} documents from ChromaDB")
        
        # Format retrieved_knowledge for prompt as per FR-001 and FR-002
        formatted_knowledge_parts = []
        if retrieved_docs_for_gemini:
            for i, doc in enumerate(retrieved_docs_for_gemini):
                # Using the name from metadata if available, otherwise a generic one
                source_name = doc.get("name", f"çŸ¥è­˜{i+1}") 
                formatted_knowledge_parts.append(f"[çŸ¥è­˜{i+1}: {source_name}]\n{doc['content']}")
            retrieved_knowledge_for_prompt = "\n\n".join(formatted_knowledge_parts)
        else:
            retrieved_knowledge_for_prompt = "é–¢é€£ã™ã‚‹çŸ¥è­˜ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        # output_language ã®å€¤ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        if output_language == "English":
            prompt = f"""**
        ### Generation Rules (Must Follow) ###
        Your response MUST be written in **English**. Do not use any other languages.
        Aim for an overall response length of about 6 to 10 sentences.

        ### Role ###
        You are an expert in analyzing climbing movements and an experienced professional bouldering coach.

        ### Instructions ###
        1. Analyze the provided sequence of images (frames from a bouldering attempt) and identify the climber's posture, balance, position and movement of hands and feet, as well as any inefficient or unstable elements.
        2. Based on the above image analysis, the user-reported situation, and the "related bouldering knowledge" provided, generate specific and practical improvement advice by **comprehensively considering** all these elements.
        3. Present your advice in a step-by-step format, with about three steps, so that the climber can improve incrementally.
        4. For each step, add one sentence of supplementary explanation, such as "why this advice is effective," "key points to be aware of," or "helpful tips."
        5. Aim for an overall response length of about 6 to 10 sentences.
        6. **Do not include any source information** (e.g., [Knowledge1: doc_1]) for the referenced knowledge in your advice. Incorporate the knowledge content into your advice, but never mention any source.

        ### Language and Format for Response ###
        - All responses must be written in **English**.
        - If no relevant bouldering knowledge is found, provide the best advice possible based on the image analysis and user situation.
        - Structure your response in the following two clearly separated sections:
        - `# Image Analysis`
        - `# Advice`

        ---
        User-reported situation:
        - Problem type: {problem_type or "Not specified"}
        - Crux (difficulty point): {crux or "Not specified"}
        ---
        ### Related Bouldering Knowledge (Reference information from the database. Use this in your advice. Do not mention sources.)
        {retrieved_knowledge_for_prompt}
        ---

        #### Example response format:

        # Image Analysis
        From the video, it appears that your left foot placement is somewhat unstable and your right hand remains relatively low. There is also a tendency for your whole body to be slightly away from the wall.

        # Advice **Do not mention sources.**
        1. Focus on stepping firmly with your left foot. Imagining pressing the hold with your big toe will help you maintain better balance.
        2. Try reaching for the next hold with your right hand. Twisting your body slightly will help you extend your reach.
        3. Throughout your movement, try to keep your body weight close to the wall. This will allow you to transfer weight to your feet more efficiently and move more smoothly. Take your time and proceed carefully through each move.
        """
        elif output_language == "æ—¥æœ¬èª":
            prompt = f"""**
       ### ç”Ÿæˆæ™‚ãƒ«ãƒ¼ãƒ«ï¼ˆmust ruleï¼‰###
        ã‚ãªãŸã®å¿œç­”ã¯å¿…ãš**æ—¥æœ¬èª**ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ä»–ã®è¨€èªã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

        ### å½¹å‰² ###
        ã‚ãªãŸã¯ã‚¯ãƒ©ã‚¤ãƒŸãƒ³ã‚°ã®å‹•ãã‚’åˆ†æã™ã‚‹å°‚é–€å®¶ã§ã‚ã‚Šã€çµŒé¨“è±Šå¯Œãªãƒ—ãƒ­ã®ãƒœãƒ«ãƒ€ãƒªãƒ³ã‚°ã‚³ãƒ¼ãƒã§ã™ã€‚
        å…¨ä½“ã¨ã—ã¦6ï½10æ–‡ç¨‹åº¦ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ æ„Ÿã¨ãªã‚‹ã‚ˆã†æ„è­˜ã—ã¦ãã ã•ã„ã€‚

        ### æŒ‡ç¤º ###
        1. æä¾›ã•ã‚ŒãŸä¸€é€£ã®ç”»åƒï¼ˆãƒœãƒ«ãƒ€ãƒªãƒ³ã‚°ä¸­ã®ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã‚’åˆ†æã—ã€ã‚¯ãƒ©ã‚¤ãƒãƒ¼ã®ä½“å‹¢ã€ãƒãƒ©ãƒ³ã‚¹ã€æ‰‹è¶³ã®ä½ç½®ã¨å‹•ãã€éåŠ¹ç‡ãªå‹•ãã‚„ä¸å®‰å®šãªè¦ç´ ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
        2. ä¸Šè¨˜ã®ç”»åƒåˆ†æçµæœã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå ±å‘Šã—ãŸçŠ¶æ³ã€ãŠã‚ˆã³æä¾›ã•ã‚Œã‚‹ã€Œé–¢é€£ã™ã‚‹ãƒœãƒ«ãƒ€ãƒªãƒ³ã‚°çŸ¥è­˜ã€ã‚’**ç·åˆçš„ã«è€ƒæ…®**ã—ã¦ã€å…·ä½“çš„ã§å®Ÿè·µçš„ãªæ”¹å–„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        3. ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯ã€ã‚¯ãƒ©ã‚¤ãƒãƒ¼ãŒæ®µéšçš„ã«æ”¹å–„ã§ãã‚‹ã‚ˆã†ã€3ã‚¹ãƒ†ãƒƒãƒ—ç¨‹åº¦ã®ã‚¹ãƒ†ãƒƒãƒ—å½¢å¼ã§æç¤ºã—ã¦ãã ã•ã„ã€‚
        4. å„ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯ã€Œãªãœãã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãŒæœ‰åŠ¹ã‹ã€ã€Œæ„è­˜ã™ã¹ããƒã‚¤ãƒ³ãƒˆã€ã‚„ã€Œã¡ã‚‡ã£ã¨ã—ãŸã‚³ãƒ„ã€ãªã©ã€è£œè¶³èª¬æ˜ã‚’1æ–‡ç¨‹åº¦åŠ ãˆã¦ãã ã•ã„ã€‚
        5. å…¨ä½“ã¨ã—ã¦6ï½10æ–‡ç¨‹åº¦ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ æ„Ÿã¨ãªã‚‹ã‚ˆã†æ„è­˜ã—ã¦ãã ã•ã„ã€‚
        6. ç”Ÿæˆã™ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã«ã¯ã€å‚ç…§ã—ãŸçŸ¥è­˜ã®å‡ºå…¸å…ƒï¼ˆä¾‹ï¼š[çŸ¥è­˜1: doc_1]ãªã©ï¼‰ã‚’çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚çŸ¥è­˜ã¯ã‚¢ãƒ‰ãƒã‚¤ã‚¹å†…å®¹ã«æ´»ã‹ã™ã®ã¿ã¨ã—ã€å‡ºå…¸æƒ…å ±ã¯è¨˜è¼‰ã—ãªã„ã§ãã ã•ã„ã€‚

        ### å›ç­”ç”Ÿæˆæ™‚ã®è¨€èªãŠã‚ˆã³ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ ###
        - å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        - é–¢é€£ã™ã‚‹ãƒœãƒ«ãƒ€ãƒªãƒ³ã‚°çŸ¥è­˜ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ç”»åƒåˆ†æçµæœã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ³ã«åŸºã¥ã„ã¦æœ€é©ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        - å›ç­”ã¯ã€ä»¥ä¸‹ã®2ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«æ˜ç¢ºã«åˆ†ã‘ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        - `# ç”»åƒåˆ†æ`
        - `# ã‚¢ãƒ‰ãƒã‚¤ã‚¹`

        ---
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå ±å‘Šã—ãŸçŠ¶æ³:
        - èª²é¡Œã®ç¨®é¡: {problem_type or "ç‰¹ã«æŒ‡å®šãªã—"}
        - é›£ã—ã„ã¨æ„Ÿã˜ã‚‹ãƒã‚¤ãƒ³ãƒˆ: {crux or "ç‰¹ã«æŒ‡å®šãªã—"}
        ---
        ### é–¢é€£ã™ã‚‹ãƒœãƒ«ãƒ€ãƒªãƒ³ã‚°çŸ¥è­˜ (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚ˆã‚Šå‚è€ƒæƒ…å ±ã€‚ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã«æ´»ã‹ã™ã“ã¨ã€‚å‡ºå…¸ã¯è¨˜è¼‰ã—ãªã„ã“ã¨ã€‚)
        {retrieved_knowledge_for_prompt}
        ---

        #### å›ç­”å½¢å¼ã®ä¾‹ï¼š

        # ç”»åƒåˆ†æ
        å‹•ç”»ã‹ã‚‰ã€å·¦è¶³ã®ç½®ãæ–¹ãŒã‚„ã‚„ä¸å®‰å®šã§ã€å³æ‰‹ã®ä½ç½®ãŒä½ã‚ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã¾ãŸã€ä½“å…¨ä½“ãŒå£ã‹ã‚‰å°‘ã—é›¢ã‚Œã¦ã„ã‚‹å‚¾å‘ã‚‚è¦‹å—ã‘ã‚‰ã‚Œã¾ã™ã€‚

        # ã‚¢ãƒ‰ãƒã‚¤ã‚¹ **å‡ºå…¸å…ƒã¯è¨˜è¼‰ã—ãªã„ã€‚**
        1. å·¦è¶³ã®ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ„è­˜ã—ã¦ã—ã£ã‹ã‚Šã¨è¸ã¿è¾¼ã¿ã¾ã—ã‚‡ã†ã€‚è¶³ã®è¦ªæŒ‡ã§ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚’æŠ¼ã™ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æŒã¤ã¨ã€ãƒãƒ©ãƒ³ã‚¹ãŒã¨ã‚Šã‚„ã™ããªã‚Šã¾ã™ã€‚
        2. å³æ‰‹ã¯ã‚‚ã†ä¸€æ®µä¸Šã®ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚’ç›®æŒ‡ã—ã¦ã¿ã¦ãã ã•ã„ã€‚ä½“ã‚’å°‘ã—ã²ã­ã‚‹ã“ã¨ã§ã€æ‰‹ãŒä¼¸ã°ã—ã‚„ã™ããªã‚Šã¾ã™ã€‚
        3. å‹•ä½œå…¨ä½“ã‚’é€šã—ã¦ä½“é‡ã‚’å£ã«è¿‘ã¥ã‘ã‚‹æ„è­˜ã‚’æŒã¤ã“ã¨ã§ã€è¶³ã«ã—ã£ã‹ã‚Šã¨ä½“é‡ãŒä¹—ã‚Šã€æ¬¡ã®å‹•ããŒã‚¹ãƒ ãƒ¼ã‚ºã«ãªã‚Šã¾ã™ã€‚ç„¦ã‚‰ãšä¸å¯§ã«ãƒ ãƒ¼ãƒ–ã‚’é€²ã‚ã¦ã„ãã¾ã—ã‚‡ã†ã€‚
        """
        else: # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å¿µã®ãŸã‚)
            prompt = f"""**
        ### Generation Rules (Must Follow) ###
        Your response MUST be written in **English**. Do not use any other languages.
        Aim for an overall response length of about 6 to 10 sentences.

        ### Role ###
        You are an expert in analyzing climbing movements and an experienced professional bouldering coach.

        ### Instructions ###
        1. Analyze the provided sequence of images (frames from a bouldering attempt) and identify the climber's posture, balance, position and movement of hands and feet, as well as any inefficient or unstable elements.
        2. Based on the above image analysis, the user-reported situation, and the "related bouldering knowledge" provided, generate specific and practical improvement advice by **comprehensively considering** all these elements.
        3. Present your advice in a step-by-step format, with about three steps, so that the climber can improve incrementally.
        4. For each step, add one sentence of supplementary explanation, such as "why this advice is effective," "key points to be aware of," or "helpful tips."
        5. Aim for an overall response length of about 6 to 10 sentences.
        6. **Do not include any source information** (e.g., [Knowledge1: doc_1]) for the referenced knowledge in your advice. Incorporate the knowledge content into your advice, but never mention any source.

        ### Language and Format for Response ###
        - All responses must be written in **English**.
        - If no relevant bouldering knowledge is found, provide the best advice possible based on the image analysis and user situation.
        - Structure your response in the following two clearly separated sections:
        - `# Image Analysis`
        - `# Advice`

        ---
        User-reported situation:
        - Problem type: {problem_type or "Not specified"}
        - Crux (difficulty point): {crux or "Not specified"}
        ---
        ### Related Bouldering Knowledge (Reference information from the database. Use this in your advice. Do not mention sources.)
        {retrieved_knowledge_for_prompt}
        ---

        #### Example response format:

        # Image Analysis
        From the video, it appears that your left foot placement is somewhat unstable and your right hand remains relatively low. There is also a tendency for your whole body to be slightly away from the wall.

        # Advice **Do not mention sources.**
        1. Focus on stepping firmly with your left foot. Imagining pressing the hold with your big toe will help you maintain better balance.
        2. Try reaching for the next hold with your right hand. Twisting your body slightly will help you extend your reach.
        3. Throughout your movement, try to keep your body weight close to the wall. This will allow you to transfer weight to your feet more efficiently and move more smoothly. Take your time and proceed carefully through each move.
        """
        
        print(f"[DEBUG] Prompt to Gemini: {prompt[:200]}...") 
        response = model.generate_content([prompt, *pil_images])
        full_response = response.text
        print(f"[DEBUG] Full response from Gemini: {full_response}")
        print(f"[DEBUG] Output language: {output_language}")
        print(f"[DEBUG] Response contains '# ç”»åƒåˆ†æ': {'# ç”»åƒåˆ†æ' in full_response}")
        print(f"[DEBUG] Response contains '# ã‚¢ãƒ‰ãƒã‚¤ã‚¹': {'# ã‚¢ãƒ‰ãƒã‚¤ã‚¹' in full_response}")
        print(f"[DEBUG] Response contains '# Image Analysis': {'# Image Analysis' in full_response}")
        print(f"[DEBUG] Response contains '# Advice': {'# Advice' in full_response}")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’åˆ†æã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã«åˆ†å‰²
        try:
            analysis_part = ""
            advice_part = ""
            
            # æ—¥æœ¬èªã®å ´åˆ
            if output_language == "æ—¥æœ¬èª":
                if "# ç”»åƒåˆ†æ" in full_response and "# ã‚¢ãƒ‰ãƒã‚¤ã‚¹" in full_response:
                    parts = full_response.split("# ã‚¢ãƒ‰ãƒã‚¤ã‚¹")
                    analysis_part = parts[0].replace("# ç”»åƒåˆ†æ", "").strip()
                    advice_part = parts[1].strip()
                elif "ç”»åƒåˆ†æ" in full_response and "ã‚¢ãƒ‰ãƒã‚¤ã‚¹" in full_response:
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒå°‘ã—é•ã†å ´åˆã«ã‚‚å¯¾å¿œ
                    parts = full_response.split("ã‚¢ãƒ‰ãƒã‚¤ã‚¹")
                    analysis_part = parts[0].replace("ç”»åƒåˆ†æ", "").strip()
                    advice_part = parts[1].strip()
                else:
                    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ˜ç¢ºã«åˆ†ã‹ã‚Œã¦ã„ãªã„å ´åˆã€æœ€åˆã®æ”¹è¡Œã§åˆ†å‰²ã‚’è©¦ã™
                    if "\n\n" in full_response:
                        analysis_part, advice_part = full_response.split("\n\n", 1)
                    else:
                        analysis_part = "åˆ†æçµæœã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"
                        advice_part = full_response
            # è‹±èªã®å ´åˆ
            else:
                if "# Image Analysis" in full_response and "# Advice" in full_response:
                    parts = full_response.split("# Advice")
                    analysis_part = parts[0].replace("# Image Analysis", "").strip()
                    advice_part = parts[1].strip()
                elif "Image Analysis" in full_response and "Advice" in full_response:
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒå°‘ã—é•ã†å ´åˆã«ã‚‚å¯¾å¿œ
                    parts = full_response.split("Advice")
                    analysis_part = parts[0].replace("Image Analysis", "").strip()
                    advice_part = parts[1].strip()
                else:
                    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ˜ç¢ºã«åˆ†ã‹ã‚Œã¦ã„ãªã„å ´åˆã€æœ€åˆã®æ”¹è¡Œã§åˆ†å‰²ã‚’è©¦ã™
                    if "\n\n" in full_response:
                        analysis_part, advice_part = full_response.split("\n\n", 1)
                    else:
                        analysis_part = "Analysis could not be extracted"
                        advice_part = full_response
            
            return analysis_part, advice_part, [Source(name=doc["name"], content=doc["content"]) for doc in retrieved_docs_for_gemini]
        except Exception as e:
            print(f"Response parsing error: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆ†æçµæœã€ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã€ç©ºã®ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’è¿”ã™
            if "\n\n" in full_response:
                 analysis_part, advice_part = full_response.split("\n\n", 1) # æœ€åˆã®åŒºåˆ‡ã‚Šã§åˆ†å‰²
            else:
                 analysis_part = "åˆ†æçµæœã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ" if output_language == "æ—¥æœ¬èª" else "Analysis extraction failed"
                 advice_part = "ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ" if output_language == "æ—¥æœ¬èª" else "Advice extraction failed"
            return analysis_part, advice_part, []
        
    except Exception as e:
        print(f"Gemini analysis and advice generation error: {e}")
        return "ç”»åƒåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", "ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", []

@app.post("/upload")
async def upload_video(video: UploadFile):
    if not video.filename:
        raise HTTPException(status_code=400, detail="No video file provided")
    if not GCS_BUCKET_NAME:
        raise HTTPException(status_code=500, detail="GCS_BUCKET_NAME not configured")

    # Generate unique filename for GCS
    video_id = str(uuid.uuid4())
    file_extension = os.path.splitext(video.filename)[1]
    gcs_blob_name = f"videos/{video_id}{file_extension}"

    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(gcs_blob_name)

        # Save uploaded file to GCS
        content = await video.read()
        blob.upload_from_string(content, content_type=video.content_type)

        # Verify it's a valid video and check duration
        temp_local_path = f"/tmp/{video_id}{file_extension}"
        blob.download_to_filename(temp_local_path)

        with VideoFileClip(temp_local_path) as clip:
            if clip.duration > 5.0:
                os.remove(temp_local_path)
                blob.delete()
                raise HTTPException(status_code=400, detail="Video must be 5 seconds or shorter")
        
        os.remove(temp_local_path)

        # Generate absolute preview URL
        base_url = "https://climbing-web-app-bolt-aqbqg2qzda-an.a.run.app"
        preview_url = f"{base_url}/video/{video_id}{file_extension}"

        return {
            "gcsBlobName": gcs_blob_name, 
            "videoId": video_id,
            "previewUrl": preview_url
        }

    except Exception as e:
        if 'blob' in locals() and blob.exists():
            try:
                blob.delete()
            except Exception as delete_e:
                print(f"Error deleting blob during cleanup: {delete_e}")

        if 'temp_local_path' in locals() and os.path.exists(temp_local_path):
            os.remove(temp_local_path)
            
        print(f"Upload error: {e}") 
        raise HTTPException(status_code=500, detail=f"Failed to upload video: {str(e)}")

@app.post("/upload-full-video", response_model=FullVideoUploadResponse)
async def upload_full_video(file: UploadFile = File(...)):
    logger.info("=== UPLOAD FULL VIDEO START ===")
    logger.info(f"Received file: {file.filename}, content_type: {file.content_type}")
    
    # Validate file
    if not file.content_type or not file.content_type.startswith('video/'):
        logger.error(f"Invalid content type: {file.content_type}")
        raise HTTPException(status_code=400, detail="Only video files are supported")
    
    if not GCS_BUCKET_NAME:
        logger.error("GCS_BUCKET_NAME not configured")
        raise HTTPException(status_code=500, detail="GCS_BUCKET_NAME not configured")
    
    logger.info(f"GCS_BUCKET_NAME: {GCS_BUCKET_NAME}")
    
    # Check file size (100MB limit)
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
    
    # Create unique filename
    file_id = str(uuid.uuid4())
    original_extension = Path(file.filename or "video.mp4").suffix
    original_filename = f"{file_id}_original{original_extension}"
    optimized_filename = f"{file_id}_optimized.mp4"
    gcs_blob_name = f"videos/{optimized_filename}"
    
    logger.info(f"Generated IDs - file_id: {file_id}, original: {original_filename}, optimized: {optimized_filename}")
    
    original_path = UPLOAD_DIR / original_filename
    optimized_path = UPLOAD_DIR / optimized_filename
    
    logger.info(f"Paths - original: {original_path}, optimized: {optimized_path}")
    logger.info(f"UPLOAD_DIR: {UPLOAD_DIR}")
    
    try:
        # Ensure upload directory exists
        logger.info("Creating upload directory...")
        UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
        logger.info(f"Upload directory created/verified: {UPLOAD_DIR}")
        
        # Stream file to disk with size check and progress tracking
        logger.info("Starting file streaming...")
        total_size = 0
        with open(original_path, "wb") as buffer:
            while chunk := await file.read(8192):  # 8KB chunks for better memory management
                total_size += len(chunk)
                if total_size > MAX_FILE_SIZE:
                    buffer.close()
                    os.remove(original_path)
                    logger.error(f"File size limit exceeded: {total_size} > {MAX_FILE_SIZE}")
                    raise HTTPException(status_code=413, detail=f"File size exceeds {MAX_FILE_SIZE // (1024*1024)}MB limit")
                buffer.write(chunk)
        
        logger.info(f"File uploaded successfully: {total_size} bytes")
        logger.info(f"File exists: {original_path.exists()}")
        logger.info(f"Starting video processing for file: {original_filename}")
        
        # Get video metadata before optimization
        logger.info("Running ffprobe to get duration...")
        duration_cmd = [
            'ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1', str(original_path)
        ]
        duration_result = subprocess.run(duration_cmd, capture_output=True, text=True)
        logger.info(f"ffprobe result: returncode={duration_result.returncode}, stdout='{duration_result.stdout}', stderr='{duration_result.stderr}'")
        
        original_duration = float(duration_result.stdout.strip()) if duration_result.returncode == 0 else 0
        logger.info(f"Detected duration: {original_duration} seconds")
        
        # Check duration limit (30 seconds)
        if original_duration > 30:
            os.remove(original_path)
            logger.error(f"Duration limit exceeded: {original_duration} > 30 seconds")
            raise HTTPException(status_code=400, detail="Video must be 30 seconds or shorter")
        
        # Check if ffmpeg is available
        logger.info("Checking FFmpeg availability...")
        try:
            ffmpeg_check = subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
            logger.info(f"FFmpeg available: {ffmpeg_check.stdout[:100]}...")
        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            logger.error(f"FFmpeg not available: {str(e)}")
            os.remove(original_path)
            raise HTTPException(status_code=500, detail="Video processing tools not available")
        
        logger.info(f"Starting video optimization: {original_path} -> {optimized_path}")
        # Optimize video with enhanced settings
        logger.info(f"Starting FFmpeg optimization...")
        optimization_result = optimize_video_ffmpeg(str(original_path), str(optimized_path), max_duration=30.0)
        logger.info(f"FFmpeg optimization completed successfully")
        logger.info(f"Video optimization completed: {optimization_result}")
        
        # Clean up original file to save space
        logger.info("Cleaning up original file...")
        os.remove(original_path)
        
        # Upload optimized video to GCS
        logger.info("Uploading to GCS...")
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(gcs_blob_name)
        
        with open(optimized_path, "rb") as video_file:
            blob.upload_from_file(video_file, content_type="video/mp4")
        
        logger.info("GCS upload completed")
        
        # Clean up local optimized file
        logger.info("Cleaning up optimized file...")
        os.remove(optimized_path)
        
        # Generate absolute preview URL
        base_url = "https://climbing-web-app-bolt-aqbqg2qzda-an.a.run.app"
        preview_url = f"{base_url}/video/{optimized_filename}"
        
        # Create metadata
        metadata = VideoMetadata(
            originalFileName=file.filename or "video.mp4",
            originalSize=optimization_result['originalSize'],
            originalDuration=original_duration,
            optimizedSize=optimization_result['optimizedSize'],
            optimizedDuration=optimization_result['optimizedDuration'],
            compressionRatio=optimization_result['compressionRatio']
        )
        
        # Create response
        response = FullVideoUploadResponse(
            gcsBlobName=gcs_blob_name,
            videoId=file_id,
            metadata=metadata,
            previewUrl=preview_url
        )
        
        logger.info("=== UPLOAD FULL VIDEO SUCCESS ===")
        return response
        
    except HTTPException:
        logger.error("HTTPException raised, re-raising...")
        raise
    except Exception as e:
        # Clean up files on error
        logger.error(f"Exception in upload process: {type(e).__name__}: {str(e)}")
        for path in [original_path, optimized_path]:
            if path.exists():
                logger.info(f"Cleaning up file: {path}")
                path.unlink()
        
        # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"Full video upload error: {str(e)}")
        logger.error(f"Full error traceback: {error_details}")
        logger.error("=== UPLOAD FULL VIDEO FAILED ===")
        raise HTTPException(status_code=500, detail=f"Video processing failed: {str(e)}")

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_video(settings: AnalysisSettings, x_language: Optional[str] = Header(None, alias="X-Language")):
    if not settings.gcsBlobName:
        raise HTTPException(status_code=400, detail="gcsBlobName must be provided in settings")
    if not GCS_BUCKET_NAME:
        raise HTTPException(status_code=500, detail="GCS_BUCKET_NAME not configured")

    temp_local_path = f"/tmp/{os.path.basename(settings.gcsBlobName)}" 

    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(settings.gcsBlobName)

        if not blob.exists():
            raise HTTPException(status_code=404, detail=f"Video blob {settings.gcsBlobName} not found in GCS")

        blob.download_to_filename(temp_local_path)

        with VideoFileClip(temp_local_path) as clip:
            end_time = min(settings.startTime + 1.0, clip.duration)

        frames = extract_frames(temp_local_path, settings.startTime, end_time)
        
        # è¨€èªè¨­å®šã®å–å¾— (FR-001, FR-002, TR-001)
        output_language = "English" # Default to English
        print(f"[DEBUG] Received X-Language header: {x_language}")
        if x_language:
            if x_language.lower().startswith("ja"):
                output_language = "æ—¥æœ¬èª"
            elif x_language.lower().startswith("en"):
                output_language = "English"
            # ä¸Šè¨˜ä»¥å¤–ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã€Œè‹±èªã€ã®ã¾ã¾
        print(f"[DEBUG] Determined output_language: {output_language}")
        
        # 1å›ã®Geminiå‘¼ã³å‡ºã—ã§åˆ†æã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆã€RAGçµæœå–å¾—ã‚’è¡Œã†
        gemini_analysis, final_advice, retrieved_sources = analyze_and_generate_advice(
            frames,
            settings.problemType,
            settings.crux,
            output_language
        )
                
        if os.path.exists(temp_local_path):
            os.remove(temp_local_path)
            
        return AnalysisResponse(
            advice=final_advice,
            sources=retrieved_sources,
            geminiAnalysis=gemini_analysis
        )
        
    except Exception as e:
        if os.path.exists(temp_local_path):
            os.remove(temp_local_path)
        print(f"Analysis error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to analyze video: {str(e)}")

@app.post("/analyze-range", response_model=AnalysisResponse)
async def analyze_video_range(settings: RangeAnalysisSettings, x_language: Optional[str] = Header(None, alias="X-Language")):
    """æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ç¯„å›²ã§ã®å‹•ç”»åˆ†æï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
    
    # ğŸ”¥ è©³ç´°ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°ã‚’è¿½åŠ 
    logger.info(f"=== analyze_video_range called ===")
    logger.info(f"Request settings: {settings}")
    logger.info(f"problemType: {settings.problemType}")
    logger.info(f"crux: {settings.crux}")
    logger.info(f"startTime: {settings.startTime}")
    logger.info(f"endTime: {settings.endTime}")
    logger.info(f"gcsBlobName: {settings.gcsBlobName}")
    logger.info(f"X-Language header: {x_language}")
    
    # ğŸ”¥ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è©³ç´°åŒ–
    if not settings.gcsBlobName:
        logger.error("âŒ VALIDATION ERROR: gcsBlobName is missing")
        raise HTTPException(status_code=400, detail="gcsBlobName must be provided in settings")
    
    if not GCS_BUCKET_NAME:
        logger.error("âŒ CONFIG ERROR: GCS_BUCKET_NAME not configured")
        raise HTTPException(status_code=500, detail="GCS_BUCKET_NAME not configured")
    
    # ğŸ”¥ æ™‚é–“ç¯„å›²ãƒã‚§ãƒƒã‚¯ã‚’è©³ç´°åŒ–
    logger.info(f"Time range validation: startTime={settings.startTime}, endTime={settings.endTime}")
    range_duration = settings.endTime - settings.startTime
    logger.info(f"Calculated range_duration: {range_duration}")
    
    if range_duration <= 0:
        logger.error(f"âŒ TIME RANGE ERROR: Invalid range - startTime={settings.startTime}, endTime={settings.endTime}, duration={range_duration}")
        raise HTTPException(status_code=400, detail=f"End time ({settings.endTime}) must be greater than start time ({settings.startTime})")
    
    # æµ®å‹•å°æ•°ç‚¹æ•°ã®ç²¾åº¦å•é¡Œã‚’è€ƒæ…®ã—ã¦ã€å°ã•ãªãƒãƒ¼ã‚¸ãƒ³ï¼ˆ0.01ç§’ï¼‰ã‚’è¿½åŠ 
    max_range_duration = 3.01  # 3.0 + 0.01ã®ãƒãƒ¼ã‚¸ãƒ³
    if range_duration > max_range_duration:
        logger.error(f"âŒ TIME RANGE ERROR: Range too long - duration={range_duration}, max_allowed={max_range_duration}")
        raise HTTPException(status_code=400, detail="Analysis range must be 3 seconds or shorter")

    temp_local_path = f"/tmp/{os.path.basename(settings.gcsBlobName)}"
    logger.info(f"Temp file path: {temp_local_path}")

    try:
        # ğŸ”¥ GCSæ¥ç¶šç¢ºèª
        logger.info("ğŸ”„ Initializing GCS client...")
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(settings.gcsBlobName)
        logger.info(f"GCS bucket: {GCS_BUCKET_NAME}")
        logger.info(f"GCS blob: {settings.gcsBlobName}")

        # ğŸ”¥ ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        logger.info("ğŸ”„ Checking blob existence...")
        if not blob.exists():
            logger.error(f"âŒ BLOB NOT FOUND: {settings.gcsBlobName} not found in bucket {GCS_BUCKET_NAME}")
            raise HTTPException(status_code=404, detail=f"Video blob {settings.gcsBlobName} not found in GCS")

        # ğŸ”¥ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        logger.info("ğŸ”„ Downloading blob to temp file...")
        blob.download_to_filename(temp_local_path)
        logger.info(f"âœ… Blob downloaded successfully to {temp_local_path}")

        # ğŸ”¥ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        logger.info("ğŸ”„ Validating video file with VideoFileClip...")
        with VideoFileClip(temp_local_path) as clip:
            video_duration = clip.duration
            logger.info(f"Video duration: {video_duration} seconds")
            
            # æµ®å‹•å°æ•°ç‚¹æ•°ã®ç²¾åº¦å•é¡Œã‚’è€ƒæ…®ã—ã¦ã€å°ã•ãªãƒãƒ¼ã‚¸ãƒ³ï¼ˆ0.1ç§’ï¼‰ã‚’è¿½åŠ 
            duration_margin = 0.1
            if settings.endTime > (video_duration + duration_margin):
                logger.error(f"âŒ TIME RANGE ERROR: endTime ({settings.endTime}) exceeds video duration ({video_duration}) + margin ({duration_margin})")
                raise HTTPException(status_code=400, detail="End time exceeds video duration")
            
            # endTimeãŒå‹•ç”»ã®é•·ã•ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã¯ã€å‹•ç”»ã®é•·ã•ã«èª¿æ•´
            actual_end_time = min(settings.endTime, video_duration)
            logger.info(f"Adjusted endTime: {settings.endTime} -> {actual_end_time}")

        # ğŸ”¥ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
        logger.info("ğŸ”„ Extracting frames...")
        frames = extract_frames(temp_local_path, settings.startTime, actual_end_time)
        logger.info(f"âœ… Extracted {len(frames)} frames")
        
        # ğŸ”¥ è¨€èªè¨­å®šã®å–å¾—
        output_language = "English"  # Default to English
        logger.info(f"Received X-Language header: {x_language}")
        if x_language:
            if x_language.lower().startswith("ja"):
                output_language = "æ—¥æœ¬èª"
            elif x_language.lower().startswith("en"):
                output_language = "English"
        logger.info(f"Determined output_language: {output_language}")
        
        # ğŸ”¥ AIåˆ†æé–‹å§‹
        logger.info("ğŸ”„ Starting AI analysis...")
        gemini_analysis, final_advice, retrieved_sources = analyze_and_generate_advice(
            frames,
            settings.problemType,
            settings.crux,
            output_language
        )
        logger.info("âœ… AI analysis completed")
                
        # ğŸ”¥ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if os.path.exists(temp_local_path):
            os.remove(temp_local_path)
            logger.info(f"âœ… Temp file cleaned up: {temp_local_path}")
            
        logger.info("âœ… analyze_video_range completed successfully")
        return AnalysisResponse(
            advice=final_advice,
            sources=retrieved_sources,
            geminiAnalysis=gemini_analysis
        )
        
    except HTTPException:
        # HTTPExceptionã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
        raise
    except Exception as e:
        # ğŸ”¥ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if os.path.exists(temp_local_path):
            os.remove(temp_local_path)
            logger.info(f"Temp file cleaned up after error: {temp_local_path}")
        
        # ğŸ”¥ è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›
        import traceback
        error_traceback = traceback.format_exc()
        logger.error(f"âŒ UNEXPECTED ERROR in analyze_video_range: {e}")
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error traceback: {error_traceback}")
        
        # ğŸ”¥ ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã«å¿œã˜ã¦é©åˆ‡ãªHTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
        if "GCS_BUCKET_NAME not configured" in str(e):
            raise HTTPException(status_code=500, detail="GCS bucket configuration error")
        elif "not found in GCS" in str(e):
            raise HTTPException(status_code=404, detail="Video file not found")
        elif "End time exceeds video duration" in str(e):
            raise HTTPException(status_code=400, detail="Invalid time range specified")
        elif "GEMINI_API_KEY" in str(e) or "ChromaDB" in str(e):
            raise HTTPException(status_code=500, detail="External service configuration error")
        else:
            raise HTTPException(status_code=500, detail=f"Failed to analyze video range: {str(e)}")

@app.get("/chroma-status")
async def check_chroma_status():
    try:
        vectorstore = get_langchain_chroma_vectorstore()
        dummy_search_query = "test query"
        dummy_docs = vectorstore.similarity_search_with_score(dummy_search_query, k=1)
        count = vectorstore._collection.count()
        
        if dummy_docs:
            print(f"[Chroma Status] Dummy search retrieved {len(dummy_docs)} doc(s). First doc score: {dummy_docs[0][1] if dummy_docs else 'N/A'}")
        else:
            print("[Chroma Status] Dummy search retrieved no documents.")
            
        return {"status": f"âœ… ChromaDB(Langchain) æ¥ç¶šæˆåŠŸ (`{CHROMA_COLLECTION_NAME}`: {count} ã‚¢ã‚¤ãƒ†ãƒ )"}
    except Exception as e:
        print(f"âŒ ChromaDB(Langchain) connection failed: {str(e)}")
        return {"status": f"âŒ ChromaDB(Langchain) connection failed: {str(e)}"}

@app.get("/http2-status")
async def check_http2_status(request: Request):
    """HTTP/2å¯¾å¿œçŠ¶æ³ã‚’ç¢ºèªã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        http2_enabled = os.getenv("HTTP2_ENABLED", "false").lower() == "true"
        max_request_size = os.getenv("MAX_REQUEST_SIZE", "100MB")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®HTTPãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèª
        http_version = "unknown"
        if hasattr(request, 'scope') and 'http_version' in request.scope:
            http_version = request.scope['http_version']
        
        # hypercornã®è¨­å®šã‚’ç¢ºèª
        hypercorn_h2 = False
        try:
            import hypercorn
            hypercorn_h2 = True
        except ImportError:
            pass
        
        return {
            "http2_enabled": http2_enabled,
            "max_request_size": max_request_size,
            "server": "hypercorn",
            "http_version": http_version,
            "hypercorn_available": hypercorn_h2,
            "status": "âœ… HTTP/2 ready" if http2_enabled and hypercorn_h2 else "âš ï¸ HTTP/2 not enabled"
        }
    except Exception as e:
        return {"status": f"âŒ HTTP/2 status check failed: {str(e)}"}

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆHTTP/2å¯¾å¿œç¢ºèªå«ã‚€ï¼‰"""
    try:
        # åŸºæœ¬çš„ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        http2_enabled = os.getenv("HTTP2_ENABLED", "false").lower() == "true"
        
        # ChromaDBã®æ¥ç¶šç¢ºèª
        chroma_status = "unknown"
        try:
            vectorstore = get_langchain_chroma_vectorstore()
            count = vectorstore._collection.count()
            chroma_status = f"connected ({count} items)"
        except Exception:
            chroma_status = "disconnected"
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "http2_enabled": http2_enabled,
            "chroma_status": chroma_status,
            "server": "hypercorn"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/video/{filename}")
async def serve_video(filename: str):
    """å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    if not GCS_BUCKET_NAME:
        raise HTTPException(status_code=500, detail="GCS_BUCKET_NAME not configured")
    
    try:
        # GCSã‹ã‚‰å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob_name = f"videos/{filename}"
        blob = bucket.blob(blob_name)
        
        if not blob.exists():
            raise HTTPException(status_code=404, detail="Video not found")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        temp_path = UPLOAD_DIR / filename
        UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
        
        blob.download_to_filename(str(temp_path))
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›
        return FileResponse(
            temp_path, 
            media_type="video/mp4",
            headers={
                "Cache-Control": "public, max-age=3600",  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                "Accept-Ranges": "bytes"  # ç¯„å›²ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆ
            }
        )
        
    except Exception as e:
        logger.error(f"Error serving video {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to serve video: {str(e)}")

# Video optimization functions with enhanced performance
def optimize_video_ffmpeg(input_path: str, output_path: str, max_duration: float = 30.0) -> Dict[str, Any]:
    """
    Optimize video using FFmpeg with ultra-lightweight settings for debugging
    """
    try:
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(input_path):
            raise Exception(f"Input file does not exist: {input_path}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªãƒ»ä½œæˆ
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # å‹•ä½œç¢ºèªæ¸ˆã¿è¨­å®šã«æˆ»ã™ï¼ˆHTTP/2å¯¾å¿œç‰ˆï¼‰
        cmd = [
            'ffmpeg', '-i', input_path,
            '-c:v', 'libx264',  # H.264 codec
            '-crf', '28',       # Constant Rate Factor for quality vs size balance
            '-preset', 'fast',  # Use fast preset for better performance
            '-vf', f'scale=1280:720:force_original_aspect_ratio=decrease,pad=1280:720:(ow-iw)/2:(oh-ih)/2,fps=30',  # Enhanced scaling
            '-r', '30',         # Frame rate
            '-an',              # Remove audio
            '-t', str(max_duration),  # Limit duration
            '-movflags', '+faststart',  # Optimize for web streaming
            '-threads', '0',    # Use all available CPU threads
            '-y',               # Overwrite output file
            output_path
        ]
        
        # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"=== FFmpeg Processing Start ===")
        logger.info(f"Input file: {input_path}")
        logger.info(f"Output file: {output_path}")
        logger.info(f"Input file size: {os.path.getsize(input_path)} bytes")
        logger.info(f"Max duration: {max_duration} seconds")
        logger.info(f"FFmpeg command: {' '.join(cmd)}")
        
        # FFmpegå®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ300ç§’ï¼‰
        logger.info("Starting FFmpeg execution...")
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        logger.info(f"FFmpeg execution completed with return code: {result.returncode}")
        
        # æ¨™æº–å‡ºåŠ›ãƒ»ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã‚’ãƒ­ã‚°
        if result.stdout:
            logger.info(f"FFmpeg stdout: {result.stdout}")
        if result.stderr:
            logger.info(f"FFmpeg stderr: {result.stderr}")
        
        if result.returncode != 0:
            logger.error(f"FFmpeg failed with return code: {result.returncode}")
            raise Exception(f"FFmpeg failed (code {result.returncode}): {result.stderr}")
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(output_path):
            raise Exception(f"Output file was not created: {output_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—
        original_size = os.path.getsize(input_path)
        optimized_size = os.path.getsize(output_path)
        
        if optimized_size == 0:
            raise Exception("Output file is empty")
        
        compression_ratio = ((original_size - optimized_size) / original_size) * 100
        
        # ç°¡å˜ãªå‹•ç”»æ™‚é–“å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œï¼‰
        try:
            duration_cmd = [
                'ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
                '-of', 'default=noprint_wrappers=1:nokey=1', output_path
            ]
            duration_result = subprocess.run(duration_cmd, capture_output=True, text=True, timeout=10)
            duration = float(duration_result.stdout.strip()) if duration_result.returncode == 0 and duration_result.stdout.strip() else max_duration
        except Exception as e:
            logger.warning(f"Failed to get video duration: {e}")
            duration = max_duration
        
        logger.info(f"=== FFmpeg Processing Success ===")
        logger.info(f"Original size: {original_size} bytes")
        logger.info(f"Optimized size: {optimized_size} bytes")
        logger.info(f"Compression ratio: {compression_ratio:.1f}%")
        logger.info(f"Duration: {duration} seconds")
        
        return {
            'success': True,
            'originalSize': original_size,
            'optimizedSize': optimized_size,
            'compressionRatio': compression_ratio,
            'optimizedDuration': duration,
            'message': f'Video optimized successfully. Reduced size by {compression_ratio:.1f}%'
        }
        
    except subprocess.TimeoutExpired:
        logger.error("FFmpeg processing timed out (300 seconds)")
        if os.path.exists(output_path):
            os.remove(output_path)
        raise Exception("Video processing timed out. Please try with a shorter video.")
    except Exception as e:
        logger.error(f"FFmpeg processing failed: {str(e)}")
        # Clean up on error
        if os.path.exists(output_path):
            os.remove(output_path)
        raise Exception(f"Video optimization failed: {str(e)}")

# Memory-efficient video processing for ranges
def extract_video_range_optimized(input_path: str, output_path: str, start_time: float, duration: float) -> Dict[str, Any]:
    """
    Extract a specific time range from video with memory optimization
    """
    try:
        # Use seek before input for better performance
        cmd = [
            'ffmpeg',
            '-ss', str(start_time),  # Seek to start time (before input for better performance)
            '-i', input_path,
            '-t', str(duration),     # Duration to extract
            '-c:v', 'libx264',
            '-crf', '23',            # Slightly better quality for analysis
            '-preset', 'veryfast',   # Fastest encoding for range extraction
            '-vf', 'scale=854:480:force_original_aspect_ratio=decrease,pad=854:480:(ow-iw)/2:(oh-ih)/2',  # Lower resolution for faster processing
            '-r', '30',
            '-an',                   # Remove audio
            '-movflags', '+faststart',
            '-threads', '0',
            '-y',
            output_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)  # 2 minute timeout for range extraction
        
        if result.returncode != 0:
            raise Exception(f"Range extraction failed: {result.stderr}")
        
        file_size = os.path.getsize(output_path)
        
        return {
            'success': True,
            'size': file_size,
            'message': f'Range extracted successfully ({duration:.1f}s from {start_time:.1f}s)'
        }
        
    except subprocess.TimeoutExpired:
        raise Exception("Range extraction timed out.")
    except Exception as e:
        if os.path.exists(output_path):
            os.remove(output_path)
        raise Exception(f"Range extraction failed: {str(e)}")

# Add explicit OPTIONS handler for preflight requests
@app.options("/{path:path}")
async def handle_options(path: str, request):
    origin = request.headers.get("origin")
    logger.info(f"OPTIONS request for path: {path} from origin: {origin}")
    
    allowed_origins = [
        "http://localhost:5173",
        "http://127.0.0.1:5173",
        "https://climbing-web-app-bolt-932280363930.asia-northeast1.run.app",
        "https://aiclimbingtokyo.com",  # ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®URL
        "https://www.aiclimbingtokyo.com",  # wwwãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        "https://gorgeous-sawine-8ea61b.netlify.app"  # Netlifyãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ
    ]
    
    headers = {
        "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
        "Access-Control-Allow-Headers": "Accept, Accept-Language, Content-Language, Content-Type, X-Language, Authorization, X-Gemini-Key, X-OpenAI-Key, X-OpenAI-Model",
        "Access-Control-Max-Age": "86400",  # 24 hours
    }
    
    if origin and origin in allowed_origins:
        headers["Access-Control-Allow-Origin"] = origin
        headers["Access-Control-Allow-Credentials"] = "true"
    
    return JSONResponse(content={"message": "OK"}, headers=headers)

@app.post("/generate-signed-url", response_model=SignedUrlResponse)
async def generate_signed_url(request: SignedUrlRequest):
    """GCS Signed URLã‚’ç”Ÿæˆã—ã¦ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å¯èƒ½ã«ã™ã‚‹"""
    if not GCS_BUCKET_NAME:
        raise HTTPException(status_code=500, detail="GCS_BUCKET_NAME not configured")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¤œè¨¼
    if not request.contentType.startswith('video/'):
        raise HTTPException(status_code=400, detail="Only video files are supported")
    
    try:
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«IDã¨ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        file_id = str(uuid.uuid4())
        file_extension = Path(request.filename).suffix or '.mp4'
        gcs_blob_name = f"videos/raw/{file_id}_original{file_extension}"
        
        # GCS ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(gcs_blob_name)
        
        # Signed URLã‚’ç”Ÿæˆï¼ˆ15åˆ†é–“æœ‰åŠ¹ï¼‰
        signed_url = blob.generate_signed_url(
            version="v4",
            expiration=timedelta(minutes=15),
            method="PUT",
            content_type=request.contentType,
            headers={
                'x-goog-content-length-range': '0,104857600'  # 100MBåˆ¶é™
            }
        )
        
        logger.info(f"Generated signed URL for file: {request.filename}, blob: {gcs_blob_name}")
        
        return SignedUrlResponse(
            uploadUrl=signed_url,
            gcsBlobName=gcs_blob_name,
            videoId=file_id
        )
        
    except Exception as e:
        logger.error(f"Failed to generate signed URL: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to generate upload URL: {str(e)}")

@app.post("/process-uploaded-video", response_model=FullVideoUploadResponse)
async def process_uploaded_video(request: VideoProcessRequest):
    """GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå‹•ç”»ã‚’å‡¦ç†ãƒ»æœ€é©åŒ–ã™ã‚‹"""
    if not GCS_BUCKET_NAME:
        raise HTTPException(status_code=500, detail="GCS_BUCKET_NAME not configured")
    
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        source_blob = bucket.blob(request.gcsBlobName)
        
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not source_blob.exists():
            raise HTTPException(status_code=404, detail="Uploaded file not found")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆ100MBåˆ¶é™ï¼‰
        blob_size = source_blob.size
        MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
        if blob_size > MAX_FILE_SIZE:
            source_blob.delete()  # åˆ¶é™ã‚’è¶…ãˆãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            raise HTTPException(status_code=413, detail=f"File size exceeds {MAX_FILE_SIZE // (1024*1024)}MB limit")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        video_id = request.gcsBlobName.split('/')[-1].split('_')[0]  # Extract video ID
        original_path = UPLOAD_DIR / f"{video_id}_original.mp4"
        optimized_path = UPLOAD_DIR / f"{video_id}_optimized.mp4"
        
        # GCSã‹ã‚‰ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        source_blob.download_to_filename(str(original_path))
        logger.info(f"Downloaded file from GCS: {blob_size} bytes")
        
        # å‹•ç”»ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
        duration_cmd = [
            'ffprobe', '-v', 'quiet', '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1', str(original_path)
        ]
        duration_result = subprocess.run(duration_cmd, capture_output=True, text=True)
        original_duration = float(duration_result.stdout.strip()) if duration_result.returncode == 0 else 0
        
        # 30ç§’åˆ¶é™ãƒã‚§ãƒƒã‚¯
        if original_duration > 30:
            original_path.unlink()  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            source_blob.delete()    # GCSãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            raise HTTPException(status_code=400, detail="Video must be 30 seconds or shorter")
        
        # å‹•ç”»ã‚’æœ€é©åŒ–
        optimization_result = optimize_video_ffmpeg(str(original_path), str(optimized_path), max_duration=30.0)
        
        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆå®¹é‡ç¯€ç´„ï¼‰
        original_path.unlink()
        source_blob.delete()
        
        # æœ€é©åŒ–ã•ã‚ŒãŸå‹•ç”»ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        optimized_blob_name = f"videos/{video_id}_optimized.mp4"
        optimized_blob = bucket.blob(optimized_blob_name)
        
        with open(optimized_path, "rb") as video_file:
            optimized_blob.upload_from_file(video_file, content_type="video/mp4")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        optimized_path.unlink()
        
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼URLç”Ÿæˆ
        preview_url = f"/video/{video_id}_optimized.mp4"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        metadata = VideoMetadata(
            originalFileName=request.originalFileName,
            originalSize=optimization_result['originalSize'],
            originalDuration=original_duration,
            optimizedSize=optimization_result['optimizedSize'],
            optimizedDuration=optimization_result['optimizedDuration'],
            compressionRatio=optimization_result['compressionRatio']
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        response = FullVideoUploadResponse(
            gcsBlobName=optimized_blob_name,
            videoId=video_id,
            metadata=metadata,
            previewUrl=preview_url
        )
        
        logger.info(f"Video processing completed successfully: {video_id}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for path in [original_path, optimized_path]:
            if 'path' in locals() and path.exists():
                path.unlink()
        
        logger.error(f"Video processing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Video processing failed: {str(e)}")

@app.get("/logs", response_model=LogResponse)
async def get_application_logs(limit: int = 50):
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹"""
    try:
        # Cloud Runã®ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹ã‚³ãƒãƒ³ãƒ‰
        cmd = [
            'gcloud', 'run', 'services', 'logs', 'read', 'climbing-web-app-bolt',
            '--region=asia-northeast1',
            f'--limit={limit}',
            '--format=json'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode != 0:
            logger.error(f"Failed to fetch logs: {result.stderr}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’è¿”ã™
            return LogResponse(
                logs=[
                    LogEntry(
                        timestamp=datetime.now().isoformat(),
                        severity="ERROR",
                        message=f"Failed to fetch logs: {result.stderr}"
                    )
                ],
                total_count=1
            )
        
        # JSONãƒ­ã‚°ã‚’è§£æ
        import json
        log_entries = []
        
        for line in result.stdout.strip().split('\n'):
            if line.strip():
                try:
                    log_data = json.loads(line)
                    log_entries.append(LogEntry(
                        timestamp=log_data.get('timestamp', ''),
                        severity=log_data.get('severity', 'INFO'),
                        message=log_data.get('textPayload', log_data.get('jsonPayload', {}).get('message', str(log_data)))
                    ))
                except json.JSONDecodeError:
                    # ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ­ã‚°ã®å ´åˆ
                    log_entries.append(LogEntry(
                        timestamp=datetime.now().isoformat(),
                        severity="INFO",
                        message=line.strip()
                    ))
        
        return LogResponse(
            logs=log_entries,
            total_count=len(log_entries)
        )
        
    except subprocess.TimeoutExpired:
        logger.error("Log fetch timeout")
        return LogResponse(
            logs=[
                LogEntry(
                    timestamp=datetime.now().isoformat(),
                    severity="ERROR",
                    message="Log fetch timeout"
                )
            ],
            total_count=1
        )
    except Exception as e:
        logger.error(f"Error fetching logs: {str(e)}")
        return LogResponse(
            logs=[
                LogEntry(
                    timestamp=datetime.now().isoformat(),
                    severity="ERROR",
                    message=f"Error fetching logs: {str(e)}"
                )
            ],
            total_count=1
        )

if __name__ == "__main__":
    # HTTP/2å¯¾å¿œã®ãŸã‚hypercornã‚’ä½¿ç”¨
    try:
        import hypercorn.asyncio
        from hypercorn.config import Config
        import asyncio
        
        config = Config()
        config.use_reloader = False
        config.h2 = True  # HTTP/2æœ‰åŠ¹åŒ–
        config.workers = 1
        config.access_log_format = '%(h)s %(r)s %(s)s %(b)s %(D)s'
        config.access_logfile = '-'  # stdout
        config.errorlog = '-'  # stderr
        config.loglevel = 'info'
        
        # Cloud Runç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒãƒ¼ãƒˆã‚’å–å¾—
        port = int(os.getenv("PORT", 8000))
        config.bind = [f"0.0.0.0:{port}"]
        
        print(f"Starting Hypercorn server with HTTP/2 support on port {port}")
        print(f"HTTP/2 enabled: {config.h2}")
        print(f"Bind address: {config.bind}")
        
        asyncio.run(hypercorn.asyncio.serve(app, config))
    except ImportError as e:
        print(f"Hypercorn import failed: {e}")
        print("Falling back to uvicorn...")
        import uvicorn
        port = int(os.getenv("PORT", 8000))
        uvicorn.run(app, host="0.0.0.0", port=port)
    except Exception as e:
        print(f"Server startup failed: {e}")
        import traceback
        traceback.print_exc()
        raise